---
title: Evaluations
sidebarTitle: Quickstart
---

Score your AI automatically. Catch regressions before production.

<Frame>
  <img src="/images/evaluations/evals.png" alt="Evaluation dashboard with scores and traces" />
</Frame>

## Why evaluations?

- Catch regressions when you change models, prompts, or tools
- Compare variants side by side with scores
- Debug failures — every datapoint links to its trace

## Run your first eval

<CodeGroup>
```typescript TypeScript
// my-first-eval.ts
import { evaluate } from '@lmnr-ai/lmnr';
import { OpenAI } from 'openai';

const client = new OpenAI();

const capitalOf = async (data: { country: string }) => {
  const response = await client.chat.completions.create({
    model: 'gpt-4o-mini',
    messages: [{ role: 'user', content: `Capital of ${data.country}? One word.` }],
  });
  return response.choices[0].message.content || '';
};

evaluate({
  data: [
    { data: { country: 'France' }, target: 'Paris' },
    { data: { country: 'Germany' }, target: 'Berlin' },
  ],
  executor: capitalOf,
  evaluators: {
    accuracy: (output, target) => (output.includes(target) ? 1 : 0),
  },
  config: {
    instrumentModules: { openAI: OpenAI },
  },
});
```

```python Python
# my_first_eval.py
from lmnr import evaluate
from openai import OpenAI

client = OpenAI()

def capital_of(data):
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"Capital of {data['country']}? One word."}]
    )
    return response.choices[0].message.content

def accuracy(output, target):
    return 1 if target in output else 0

evaluate(
    data=[
        {"data": {"country": "France"}, "target": "Paris"},
        {"data": {"country": "Germany"}, "target": "Berlin"},
    ],
    executor=capital_of,
    evaluators={"accuracy": accuracy},
)
```
</CodeGroup>

Run it:

<CodeGroup>
```bash TypeScript
LMNR_PROJECT_API_KEY=your_key npx lmnr eval my-first-eval.ts
```

```bash Python
LMNR_PROJECT_API_KEY=your_key lmnr eval my_first_eval.py
```
</CodeGroup>

## What you'll see

The CLI prints a link to your evaluation run:

```
✓ Evaluation complete
  View results: https://lmnr.ai/project/xxx/evals/yyy
```

<Frame>
  <img src="/images/evaluations/simple-eval-example.png" alt="Eval results with trace links" />
</Frame>

Click any row to see the full trace for that datapoint.

## How it works

1. **Dataset** — list of inputs (`data`) and expected outputs (`target`)
2. **Executor** — your function that produces output (LLM call, agent, etc.)
3. **Evaluators** — functions that score the output (return 0-1)

Laminar runs your executor on each datapoint, scores the output, and records a trace for every run.

## Next steps

<CardGroup cols={2}>
  <Card title="Use hosted evaluators" href="/evaluations/online-evaluators/scoring-with-hosted-evaluators">
    LLM-as-judge without writing code
  </Card>
  <Card title="Reuse datasets" href="/evaluations/using-dataset">
    Load datasets from your project
  </Card>
  <Card title="Run in CI" href="/evaluations/configuration">
    Block deployments on eval failures
  </Card>
  <Card title="Human review" href="/evaluations/human-evaluators">
    Add human labels to the loop
  </Card>
</CardGroup>
