---
title: Run your first Laminar evaluation
sidebarTitle: Quickstart
---
import GetProjectApiKey from '/snippets/get-project-api-key.mdx';

Run a complete evaluation from inputs to scores with trace links in under 5 minutes.

<Callout>
Embed placeholder: short GIF showing `lmnr eval my-first-evaluation.ts` and the resulting dashboard with clickable traces.
</Callout>

## What you'll build

- A dataset with a couple of datapoints.  
- An executor that calls your model.  
- An evaluator that scores the output.  
- A dashboard view with scores + trace links for each datapoint.

## Why this matters

- Catch regressions when you change models, prompts, or tools.  
- Compare variants side by side with scores and trace context.  
- Keep evals close to production by linking every datapoint to its trace.

## Prerequisites

<GetProjectApiKey />

Set `LMNR_PROJECT_API_KEY` in your environment.

## Copy/paste/run

<Tabs>
<Tab title="TypeScript">

```bash
npm install @lmnr-ai/lmnr openai
```

Create `my-first-evaluation.ts`:

```ts my-first-evaluation.ts
import { evaluate } from '@lmnr-ai/lmnr';
import { OpenAI } from 'openai';

const client = new OpenAI();

const capitalOfCountry = async (data: { country: string }) => {
  const response = await client.chat.completions.create({
    model: 'gpt-4o-mini',
    messages: [
      {
        role: 'user',
        content: `What is the capital of ${data.country}? Respond in one word.`,
      },
    ],
  });
  return response.choices[0].message.content || '';
};

evaluate({
  data: [
    { data: { country: 'France' }, target: 'Paris' },
    { data: { country: 'Germany' }, target: 'Berlin' },
  ],
  executor: capitalOfCountry,
  evaluators: {
    accuracy: (output, target) => (!target ? 0 : output.includes(target) ? 1 : 0),
  },
  config: {
    instrumentModules: { openAI: OpenAI },
  },
});
```

Run it:

```bash
LMNR_PROJECT_API_KEY=your_key npx lmnr eval my-first-evaluation.ts
```

</Tab>
<Tab title="Python">

```bash
pip install "lmnr[openai]" openai
```

Create `my-first-evaluation.py`:

```python my-first-evaluation.py
from lmnr import evaluate
from openai import OpenAI

client = OpenAI()

def capital_of_country(data: dict) -> str:
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"What is the capital of {data['country']}? Answer in one word."}],
    )
    return resp.choices[0].message.content or ""

def accuracy(output: str, target: str | None) -> int:
    if not target:
        return 0
    return 1 if target in output else 0

evaluate(
    data=[
        {"data": {"country": "France"}, "target": "Paris"},
        {"data": {"country": "Germany"}, "target": "Berlin"},
    ],
    executor=capital_of_country,
    evaluators={"accuracy": accuracy},
)
```

Run it:

```bash
LMNR_PROJECT_API_KEY=your_key lmnr eval my-first-evaluation.py
```

</Tab>
</Tabs>

<Note>
No explicit initialization needed. `evaluate` wires up tracing automatically. Passing `instrumentModules` (TS) or installing extras (Python) ensures your model SDK calls are traced alongside the evaluator.
</Note>

## What you should see

- CLI prints a link to the evaluation run.  
- Dashboard shows scores per datapoint.  
- Each row links to a trace; open it to inspect inputs, outputs, tokens, and cost.

<Callout>
Embed placeholder: screenshot of eval results table with a trace link, plus an open trace showing executor + evaluator spans.
</Callout>

## Build this next

- Use hosted graders → [Scoring with hosted evaluators](/evaluations/online-evaluators/scoring-with-hosted-evaluators)  
- Reuse datasets → [Using datasets](/evaluations/using-dataset)  
- Track improvements over time → pass `groupName` and view trend charts  
- Turn traces into labeled data → [Labeling queues](/queues/quickstart)
