---
sidebarTitle: Introduction
title: Ship reliable AI with Laminar evaluations
---

Evals in Laminar give you a fast way to score prompts, agents, and pipelines, then drill into every datapoint via traces. See what you can do and why it matters before diving into reference details.

<Callout>
Hero placeholder: embed a dashboard screenshot/GIF showing eval scores with clickable trace links.
</Callout>

## What you'll build

- An evaluation that runs your executor (prompt/agent/code) on a dataset.  
- Scoring via simple functions or hosted graders.  
- Results you can sort/filter, with trace links for every datapoint.

## Copy/paste/run (minimal setup)

1. Install Laminar + your model SDK.  
2. Write one `evaluate` call with your executor + evaluator.  
3. Run `lmnr eval ...` (CLI finds the evaluate call).  
4. Click the dashboard link to inspect scores and traces.

See the [Evaluations quickstart](/evaluations/quickstart) for full code in TypeScript and Python.

## Why evaluations?

- **Guardrails for change**: catch regressions when you update models, prompts, or tools.  
- **Compare variants**: run side-by-side prompts/agents and sort by scores.  
- **Trace-first debugging**: every datapoint has a trace so you can see why it passed/failed.  
- **CI-friendly**: wire into pipelines to block risky changes.

## How Laminar structures evals

- **Dataset**: list of datapoints with `data` (input), optional `target`, and metadata for filtering.  
- **Executor**: your code that produces an output (LLM call, agent step, business logic).  
- **Evaluator(s)**: functions or hosted graders that score outputs; can return one or many numeric scores.  
- **Groups**: label related runs to track progress over time.  
- **Tracing**: automatically records executor + evaluator spans with inputs/outputs.

## Build this next

- Run your first eval in 5 minutes → [Quickstart](/evaluations/quickstart)  
- Use hosted graders → [Online evaluators](/evaluations/online-evaluators/introduction)  
- Create/share datasets → [Datasets](/datasets/introduction)  
- Pipe results into dashboards → [Custom dashboards](/custom-dashboards/overview)
