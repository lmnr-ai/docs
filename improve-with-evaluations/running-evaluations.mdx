---
title: Running Evaluations
sidebarTitle: Running Evaluations
description: Execute evaluations with executors, evaluators, and CLI/SDK options
---

## Quickstart

import GetProjectApiKey from '/snippets/get-project-api-key.mdx';

This guide will walk you through running your first evaluation using Laminar's evaluation system.

Laminar provides a structured approach to create, run, and track your AI system's performance through these key components:

- **Executors** - Functions that process inputs and produce outputs, such as prompt templates, LLM calls, or production logic
- **Evaluators** - Functions that assess outputs against targets or quality criteria, producing numeric scores
- **Datasets** - Collections of datapoints (test cases) with 3 key elements:
  - `data` - Required JSON input sent to the executor
  - `target` - Optional reference data sent to the evaluator, typically containing expected outputs
  - `metadata` - Optional metadata. This can be used to filter evaluation results in the UI after the evaluation is run.
- **Visualization** - Tools to track performance trends and detect regressions over time
- **Tracing** - Automatic recording of execution flow and model invocations

Example datapoint:
```json5
{
  "data": {
    "question": "What is the capital of France? Respond in one word.",
  },
  "target": {
    "answer": "Paris"
  },
  "metadata": {
    "category": "geography"
  }
}
```

**Evaluation Groups** group related evaluations to assess one feature or component, with results aggregated for comparison.

## Evaluation Lifecycle

For each datapoint in a dataset:

1. The executor receives the `data` as input
2. The executor runs and its output is stored
3. Both the executor output and `target` are passed to the evaluator
4. The evaluator produces either a numeric score or a JSON object with multiple numeric scores
5. Results are stored and can be visualized to track performance over time

This approach helps you continuously measure your AI system's performance as you make changes, showing the impact of model updates, prompt revisions, and code changes.

### Evaluation function types

Each executor takes in the `data` as it is defined in the datapoints.
Evaluator accepts the output of the executor as its first argument,
and `target` as it's defined in the datapoints as the second argument.

This means that the type of the `data` fields in your datapoints must
match the type of the first parameter of the executor function. Similarly,
the type of the `target` fields in your datapoints must match the type of
the second parameter of the evaluator function(s).

Python is a bit more permissive. If you see type errors in TypeScript,
make sure the data types and the parameter types match.

For a more precise description, here's the partial TypeScript type signature of the `evaluate` function:

```javascript
evaluate<D, T, O>(
  data: {
    data: D,
    target?: T,
  },
  executor: (data: D, ...args: any[]) => O | Promise<O>;
  evaluators: {
    [key: string]: (output: O, target?: T, ...args: any[]) => 
      number | { [key: string]: number }
  },
  // ... other parameters
)
```

See full reference [here](/evaluations/reference#typescript-evaluation-types).

## Create your first evaluation

### Prerequisites

<GetProjectApiKey />

### Create an evaluation file

<Tabs>
<Tab title = "TypeScript">

Create a file named `my-first-evaluation.ts` and add the following code:

```javascript my-first-evaluation.ts
import { evaluate } from '@lmnr-ai/lmnr';
import { OpenAI } from 'openai';  

const client = new OpenAI();

const capitalOfCountry = async (data: {country: string}) => {
  // replace this with your LLM call or custom logic
  const response = await client.chat.completions.create({
    model: 'gpt-4.1-nano',
    messages: [
      {
        role: 'user',
        content: `What is the capital of ${data['country']}? ` +
          'Answer only with the capital, no other text.'
      }
    ]
  });
  return response.choices[0].message.content || '';
}

evaluate({
  data: [
    {
      data: { country: 'France' },
      target: 'Paris',
    },
    {
      data: { country: 'Germany' },
      target: 'Berlin',
    },
  ],
  executor: capitalOfCountry,
  evaluators: {
    accuracy: (output: string, target: string | undefined): number => {
      if (!target) return 0;
      return output.includes(target) ? 1 : 0;
    } 
  },
  config: { 
    instrumentModules: {
      openAI: OpenAI
    }
  }
})
```

<Note>
It is important to pass the `config` object with `instrumentModules` to `evaluate` to ensure that the OpenAI client and any other instrumented modules are instrumented.
</Note>
</Tab>
<Tab title = "Python">

Create a file named `my-first-evaluation.py` and add the following code:

```python my-first-evaluation.py
from lmnr import evaluate
from openai import OpenAI

client = OpenAI()

def capital_of_country(data: dict) -> str:
    # replace this with your LLM call or custom logic
    return client.chat.completions.create(
        model="gpt-4.1-nano",
        messages=[
            {
                "role": "user",
                "content": f"Generate a poem about {data['country']}. " +
                    "Answer only with the poem, no other text."
            }
        ]
    ).choices[0].message.content

def accuracy(output: str, target: str) -> int:
    return 1 if target in output else 0

evaluate(
    data=[
        {
            "data": {"country": "France"},
            "target": "Paris"
        },
        {
            "data": {"country": "Germany"},
            "target": "Berlin"
        },
    ],
    executor=capital_of_country,
    evaluators={"accuracy": accuracy}
)
```
</Tab>
</Tabs>

### Run the evaluation

You can run evaluations in two ways: using the `lmnr eval` CLI or directly executing the evaluation file.

#### Using the CLI

The Laminar CLI automatically detects top-level `evaluate` function calls in your files - you don't need to wrap them in a `main` function or any special structure.

<Tabs>
<Tab title = "TypeScript">
```sh
export LMNR_PROJECT_API_KEY=<YOUR_PROJECT_API_KEY>
npx lmnr eval my-first-evaluation.ts
```

To run multiple evaluations, place them in an `evals` directory with the naming pattern `*.eval.{ts,js}`:

```
├─ src/
├─ evals/
│  ├── my-first-evaluation.eval.ts
│  ├── my-second-evaluation.eval.ts
│  ├── ...
```

Then run all evaluations with a single command:
```sh
npx lmnr eval
```
</Tab>
<Tab title = "Python">
```sh
# 1. Make sure `lmnr` is installed in a virtual environment
# lmnr --help
# 2. Run the evaluation
export LMNR_PROJECT_API_KEY=<YOUR_PROJECT_API_KEY>
lmnr eval my-first-evaluation.py
```

To run multiple evaluations, place them in an `evals` directory with the naming pattern `eval_*.py` or `*_eval.py`:

```
├─ src/
├─ evals/
│  ├── eval_first.py
│  ├── second_eval.py
│  ├── ...
```

Then run all evaluations with a single command:
```sh
lmnr eval
```
</Tab>
</Tabs>

#### Running as a standalone script

You can also import and call `evaluate` directly from your application code:

<Tabs>
<Tab title = "TypeScript">
```bash
ts-node my-first-evaluation.ts
# or
npx tsx my-first-evaluation.ts
```
</Tab>
<Tab title = "Python">
```bash
python my-first-evaluation.py
```
</Tab>
</Tabs>

The `evaluate` function is flexible and can be used both in standalone scripts processed by the CLI and integrated directly into your application code.

<Tip>
Evaluator functions must return either a single numeric score or a JSON object where each key is a score name and the value is a numeric score.
</Tip>

<Note>
No need to initialize Laminar - `evaluate` automatically initializes Laminar behind the scenes. All instrumented function calls and model invocations are traced without any additional setup.
</Note>

### View evaluation results

When you run an evaluation from the CLI, Laminar will output the link to the dashboard where you can view the evaluation results.

Laminar stores every evaluation result. A run for every datapoint is represented as a trace. You can view the results and corresponding traces in the evaluations page.

<Frame>
  <img
    height="300"
    src="/images/evaluations/simple-eval-example.png"
    alt="Example evaluation"
  />
</Frame>

## Tracking evaluation progress

To track the score progression over time or compare evaluations side-by-side, you need to group them together. This can be achieved by passing the `groupName` parameter to the `evaluate` function.

<Tabs>
<Tab title="TypeScript">
```javascript {7}
import { evaluate, LaminarDataset } from '@lmnr-ai/lmnr';

evaluate({
    data: new LaminarDataset("name_of_your_dataset"),
    executor: yourExecutorFunction,
    evaluators: {evaluatorName: yourEvaluator},
    groupName: "evals_group_1",
});
```
</Tab>
<Tab title="Python">
```python {9}
from lmnr import evaluate, LaminarDataset
import os

evaluate(
    data=LaminarDataset("name_of_your_dataset"),
    executor=your_executor_function,
    evaluators={"evaluator_name": your_evaluator},
    project_api_key=os.environ["LMNR_PROJECT_API_KEY"],
    group_name="evals_group_1",
    # ... other optional parameters
)
```
</Tab>
</Tabs>

<Frame>
  <img
    height="300"
    src="/images/evaluations/eval-progress.png"
    alt="Example evaluation progression"
  />
</Frame>

## Configuration

## Configuring evaluations to report results to locally self-hosted Laminar

In this example, we configure the evaluation to report results to a locally self-hosted Laminar instance.

Evaluations send data to Laminar over both HTTP and gRPC. HTTP is used to create an evaluation and report
the datapoints, stats, and trace ids. OpenTelemetry traces themselves are sent over gRPC.

Assuming you have configured Laminar to run on ports 8000 and 8001 on your `localhost`, you will need to
pass these values to the `evaluate` function.

<Tabs>
<Tab title = "JavaScript/TypeScript">
```javascript
import { evaluate } from '@lmnr-ai/lmnr';
evaluate({
    data: evaluationData,
    executor: async (data) => await getCapital(data),
    evaluators: [evaluator],
    config: {
        projectApiKey: process.env.LMNR_PROJECT_API_KEY,
        baseUrl: 'http://localhost',
        httpPort: 8000,
        grpcPort: 8001,
    }
})
```
Run this file either by executing it, or by running it with `npx lmnr eval` CLI.
</Tab>
<Tab title = "Python">
```python
from lmnr import evaluate
evaluate(
    data=data,
    executor=get_capital,
    evaluators={'check_capital_correctness': evaluator},
    project_api_key=os.environ["LMNR_PROJECT_API_KEY"],
    base_url="http://localhost",
    http_port=8000,
    grpc_port=8001,
)
```
Run this file either by executing it, or by running it with `lmnr eval` CLI.

</Tab>
</Tabs>

## Configuring evaluations

### `evaluate` reference

Evaluations in Laminar are configured using the `evaluate` function. The function takes the following arguments:
- `data`: Either (1) A list of dictionaries, where each dictionary contains the data, target, and metadata for a single evaluation; or
(2) An instance of `LaminarDataset` – read more in [the dedicated page](/evaluations/using-dataset).
- `executor`: An optionally async function that takes a single argument, the evaluation data, and returns the output.
- `evaluators`: A dictionary of async functions that take the output and target as arguments and return a score.
- `name` (optional): Evaluation name, so it is easier to identify the evaluation in the UI. If not provided, a random name is assigned in the backend.
- `groupName`/`group_name` (optional): An optional string that groups evaluations together. Only evaluations with the same group name can be visually compared.

Additional optional configuration parameters are passed as a `config` object in JavaScript/TypeScript and directly to `evaluate` in Python.

<Tabs>
<Tab title="JavaScript/TypeScript">
- `projectApiKey`: The API key of the project where the evaluation results will be stored.
Required, unless you set the `LMNR_PROJECT_API_KEY` environment variable.
- `concurrencyLimit`: The number of evaluations to run in parallel. Default is `5`.
- `baseUrl`: The base URL of the Laminar instance. Do NOT include port here. Default is `https://api.lmnr.ai`.
- `httpPort`: The port of the Laminar instance for HTTP. Used to send evaluation results and metadata.
Default is 443. For local self-hosted Laminar, use 8000.
- `grpcPort`: The port of the Laminar instance for gRPC. Used to send traces via OTel gRPC exporter.
Default is 8443. For local self-hosted Laminar, use 8001.
- `instrumentModules`: An object with modules to instrument.
Read more in the [instrumentation guide](/ship-to-production/automatic-instrumentation#instrument-specific-modules-only).
</Tab>
<Tab title="Python">
- `project_api_key`: The API key of the project where the evaluation results will be stored.
Required, unless you set the `LMNR_PROJECT_API_KEY` environment variable.
- `batch_size`: The number of evaluations to run in parallel. Default is `5`.
- `base_url`: The base URL of the Laminar instance. Do NOT include port here. Default is `https://api.lmnr.ai`.
- `http_port`: The port of the Laminar instance for HTTP. Used to send evaluation results and metadata.
Default is 443. For local self-hosted Laminar, use 8000.
- `grpc_port`: The port of the Laminar instance for gRPC. Used to send traces via OTel gRPC exporter.
Default is 8443. For local self-hosted Laminar, use 8001.
- `instrument_modules`: A set of modules to instrument.
Read more in the [instrumentation guide](/ship-to-production/automatic-instrumentation#instrument-specific-modules-only).
</Tab>
</Tabs>

### eval CLI reference

`lmnr eval` subcommand is used to run evaluations.

```bash
lmnr eval [options]
# or in most Node settings
npx lmnr eval [options]
```

#### Formatting evaluation files

When you run an evaluation file from the CLI, the evaluate functions must be called
at the top level of the file. You can have one or more `evaluate` calls in the file.

<Note>
In Python, you can not `await` the calls to `evaluate` when running an evaluation file from the CLI.
</Note>

#### Options

First positional argument is the path to the evaluation file. E.g.

```bash
lmnr eval ./evals/my_evaluation.eval.ts
```

If file is not provided, `lmnr eval` will run all files in the `evals` directory that match the naming pattern.
For TypeScript/JavaScript, the pattern is `*.eval.{ts,js}`. For Python, the pattern is `eval_*.py` or `*_eval.py`.

#### Params

`--fail-on-error` – if set, the CLI will fail on non-critical errors, for example, if `evaluate` is not called in the file. Default is `false`.

## Using LaminarDataset

## Prerequisites

Have a dataset uploaded to Laminar, or collected from traces. See [datasets](/improve-with-evaluations/datasets) for more information.

## Defining data

To run an evaluation with a Laminar dataset, you pass the dataset object as `data` instead of a list of dictionaries.

Use `LaminarDataset` to create a dataset object. The dataset name should match the name of the dataset in Laminar.
The constructor also takes an optional `fetch_size`/`fetchSize` parameter, which specifies the number of datapoints to fetch at once.
The default value is 25. We strongly recommend setting this value to a number that is a multiple of the
evaluation [batch size](/evaluations/configuration#configuring-evaluations) for best performance.

<Tabs>
<Tab title="JavaScript/TypeScript">
```javascript
import { evaluate, LaminarDataset } from '@lmnr-ai/lmnr';
const data = new LaminarDataset("name_of_your_dataset");
evaluate({
    data,
    executor: yourExecutorFunction,
    evaluators: yourEvaluators,
    config: {
        projectApiKey: process.env.LMNR_PROJECT_API_KEY,
        // ... other optional parameters
    }
})
```
</Tab>
<Tab title="Python">
```python
from lmnr import evaluate, LaminarDataset
data = LaminarDataset("name_of_your_dataset")
evaluate(
    data=data,
    executor=your_executor_function,
    evaluators=your_evaluators,
    project_api_key=os.environ["LMNR_PROJECT_API_KEY"],
    # ... other optional parameters
)
```
</Tab>
</Tabs>

## Technical details and extension

`LaminarDataset` is an implementation of an abstract class `EvaluationDataset` which defines 2 methods besides initialization:
- `__len__` (`size` in JS): Returns the number of datapoints in the dataset.
- `__getitem__` (`get` in JS): Returns a single datapoint by index.

We also implement a concrete `slice` method to make slicing easier than using `__getitem__` directly.

This is inspired by the PyTorch [`Dataset` class](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files),
and is designed to be used in a similar way.

You can re-use the `EvaluationDataset` class to create your own dataset classes, for example, to fetch data from a database or an API.

<Tabs>
<Tab title="JavaScript/TypeScript">
```javascript
import { EvaluationDataset } from '@lmnr-ai/lmnr';

class MyCustomDataset extends EvaluationDataset {
    constructor(customProperty) {
        super();
        // Your custom initialization code here
    }

    public async size() {
        // Your custom implementation here
        return 0;
    }

    public async get(index: number) {
        // Your custom implementation here
        return { data: {}, target: {} };
    }

    // Optionally, you can implement other custom methods here
}
```
</Tab>
<Tab title="Python">
```python
from lmnr import EvaluationDataset

class MyCustomDataset(EvaluationDataset):
    def __init__(self, custom_property):
        super().__init__()
        # Your custom initialization code here
    
    def __len__(self):
        # Your custom implementation here
        return 0
    
    def __getitem__(self, index):
        # Your custom implementation here
        return Datapoint(data={}, target={})

    # Optionally, you can implement other custom methods here
```
</Tab>
</Tabs>

## Reference

## `evaluate` reference

Evaluations in Laminar are configured using the `evaluate` function. The function takes the following arguments:
- `data`: Either (1) A list of dictionaries, where each dictionary contains the data and target for a single evaluation; or
(2) An instance of `LaminarDataset` – read more in [the dedicated section](/evaluations/configuration#using-a-laminar-dataset-for-evaluations).
- `executor`: An optionally async function that takes a single argument, the evaluation data, and returns the output.
- `evaluators`: A dictionary of async functions that take the output and target as arguments and return a score. Keys in the dictionary are the names of the evaluators.
- `name` (optional): Evaluation name, so it is easier to identify the evaluation in the UI. If not provided, a random name is assigned.
- `groupName`/`group_name` (optional): An optional string that groups evaluations together. Only evaluations with the same group name can be visually compared.

Additional optional configuration parameters are passed as a `config` object in JavaScript/TypeScript and directly to `evaluate` in Python.

<Tabs>
<Tab title="JavaScript/TypeScript">
- `projectApiKey`: The API key of the project where the evaluation results will be stored.
Required, unless you set the `LMNR_PROJECT_API_KEY` environment variable.
- `concurrencyLimit`: The number of evaluations to run in parallel. Default is `5`.
- `baseUrl`: The base URL of the Laminar instance. Do NOT include port here. Default is `https://api.lmnr.ai`.
- `httpPort`: The port of the Laminar instance for HTTP. Used to send evaluation results and metadata.
Default is 443. For local self-hosted Laminar, use 8000.
- `grpcPort`: The port of the Laminar instance for gRPC. Used to send traces via OTel gRPC exporter.
Default is 8443. For local self-hosted Laminar, use 8001.
- `instrumentModules`: A map from module names to packages to be instrumented.
Read more in the [instrumentation guide](/ship-to-production/automatic-instrumentation#instrument-specific-modules-only).
</Tab>
<Tab title="Python">
- `project_api_key`: The API key of the project where the evaluation results will be stored.
Required, unless you set the `LMNR_PROJECT_API_KEY` environment variable.
- `concurrency_limit`: The number of evaluations to run in parallel. Default is `5`.
- `base_url`: The base URL of the Laminar instance. Do NOT include port here. Default is `https://api.lmnr.ai`.
- `http_port`: The port of the Laminar instance for HTTP. Used to send evaluation results and metadata.
Default is 443. For local self-hosted Laminar, use 8000.
- `grpc_port`: The port of the Laminar instance for gRPC. Used to send traces via OTel gRPC exporter.
Default is 8443. For local self-hosted Laminar, use 8001.
- `instrument_modules`: A set of modules to instrument.
Read more in the [instrumentation guide](/ship-to-production/automatic-instrumentation#instrument-specific-modules-only).
</Tab>
</Tabs>

### TypeScript evaluation types

Here's the full type signature of the evaluator function:
```javascript [expandable]
abstract class Dataset<D, T> {
  public async slice(start: number, end: number): Promise<Datapoint<D, T>[]> {
    const result = [];
    for (let i = Math.max(start, 0); i < Math.min(end, await this.size()); i++) {
      result.push(await this.get(i));
    }
    return result;
  }
  public abstract size(): Promise<number> | number;
  public abstract get(index: number): Promise<Datapoint<D, T>> | Datapoint<D, T>;
}

type EvaluatorFunctionReturn = number | Record<string, number>;
type EvaluatorFunction<O, T> = (output: O, target?: T, ...args: any[]) =>
  EvaluatorFunctionReturn | Promise<EvaluatorFunctionReturn>;

async function evaluate<D, T, O>({
  data, executor, evaluators, groupName, name, config,
}: {
  /**
   * List of data points to evaluate. `data` is the input to the executor
   * function,
   * `target` is the input to the evaluator function.
   * `Dataset` is the base class for `LaminarDataset`
   */
  data: (Datapoint<D, T>[]) | Dataset<D, T>;
  /**
   * The executor function. Takes the data point + any additional arguments
   * and returns the output to evaluate.
   */
  executor: (data: D, ...args: any[]) => O | Promise<O>;
  /**
   * Evaluator functions and names. Each evaluator function takes the output of
   * the executor _and_ the target data, and returns a score. The score can be a
   * single number or a dict of string keys and number values. If the score is a
   * single number, it will be named after the evaluator function. Evaluator
   * function names must contain only letters, digits, hyphens, underscores,
   * or spaces.
   */
  evaluators: Record<string, EvaluatorFunction<O, T>>;
  /**
   * Name of the evaluation. If not provided, a random name will be assigned.
   */
  name?: string;
  /**
   * Optional group id of the evaluation. Only evaluations within the same
   * group_id can be visually compared. Defaults to "default".
   */
  groupName?: string;
  /**
   * Optional override configurations for the evaluator.
   */
  config?: EvaluatorConfig;
}): Promise<void>;

/**
 * Configuration for the Evaluator
 */
interface EvaluatorConfig {
  /**
   * The number of data points to evaluate in parallel at a time. Defaults to 5.
   */
  concurrencyLimit?: number;
  /**
   * The project API key to use for the evaluation. If not provided,
   * the API key from the environment variable `LMNR_PROJECT_API_KEY` will be used.
   */
  projectApiKey?: string;
  /**
   * The base URL of the Laminar API. If not provided, the default is
   * `https://api.lmnr.ai`. Useful with self-hosted Laminar instances.
   * Do NOT include the port in the URL, use `httpPort` and `grpcPort` instead.
   */
  baseUrl?: string;
  /**
   * The HTTP port of the Laminar API. If not provided, the default is 443.
   */
  httpPort?: number;
  /**
   * The gRPC port of the Laminar API. If not provided, the default is 8443.
   */
  grpcPort?: number;
  /**
   * Object with modules to instrument. If not provided, all
   * available modules are instrumented.
   * See {@link https://docs.lmnr.ai/ship-to-production/automatic-instrumentation}
   */
  instrumentModules?: InitializeOptions['instrumentModules'];
  /**
   * If true, then the spans will not be batched.
   */
  traceDisableBatch?: boolean;
  /**
   * Timeout for trace export. Defaults to 30_000 (30 seconds), which is over
   * the default OTLP exporter timeout of 10_000 (10 seconds).
   */
  traceExportTimeoutMillis?: number;
  /**
   * Defines default log level for SDK and all instrumentations.
   */
  logLevel?: "debug" | "info" | "warn" | "error";

  /**
   * Maximum number of spans to export at a time. Defaults to 64.
   */
  traceExportBatchSize?: number;
}
```

Example use:

```javascript [expandable]
import type {
  Datapoint,
  EvaluatorFunction,
} from '@lmnr-ai/lmnr';

type DataType = {
  question: string;
  user_id: string;
  timestamp: string;
}

type TargetType = {
  answer: string;
  reasoning?: string;
}

type OutputType = string;

const accuracy: EvaluatorFunction<OutputType, TargetType> =
  (output, target) => output === target.answer ? 1 : 0;

const data: Datapoint<DataType, TargetType> = {
  input: {
    question: "What is the capital of France?",
    user_id: "123",
    timestamp: "2021-01-01",
  },
  target: { answer: "Paris" },
}

evaluate<DataType, TargetType, OutputType>({
  data,
  executor: async (data) => {
    // Call an LLM to answer the question
    return "Paris";
  },
  evaluators: { accuracy },
});
```

## eval CLI reference

`lmnr eval` subcommand is used to run evaluations.

```bash
lmnr eval [options]
# or in most Node settings
npx lmnr eval [options]
```

### Options

First positional argument is the path to the evaluation file. E.g.

```bash
lmnr eval ./evals/my_evaluation.eval.ts
```

If file is not provided, `lmnr eval` will run all files in the `evals` directory that match the naming pattern.
For TypeScript/JavaScript, the pattern is `*.eval.{ts,js}`. For Python, the pattern is `eval_*.py` or `*_eval.py`.

### Params

`--fail-on-error` – if set, the CLI will fail on non-critical errors, for example, if `evaluate` is not called in the file. Default is `false`.

## Cookbook

## Basic correctness evaluation

In this example our executor function calls an LLM to get the capital of a country. 
We then evaluate the correctness of the prediction by checking for exact match with the target capital.

<Steps>
<Step title="1. Define an executor function">

The executor function calls OpenAI to get the capital of a country.
The prompt also asks to only name the city and nothing else. In a real scenario,
you will likely want to use structured output to get the city name only.

<Tabs>
<Tab title = "JavaScript/TypeScript">
```javascript
import OpenAI from 'openai';

const openai = new OpenAI({apiKey: process.env.OPENAI_API_KEY});

const getCapital = async (
    {country}: {country: string}
): Promise<string> => {
    const response = await openai.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: [
            {
                role: 'system',
                content: 'You are a helpful assistant.'
            }, {
                role: 'user',
                content: `What is the capital of ${country}?` +
                ' Just name the city and nothing else'
            }
        ],
    });
    return response.choices[0].message.content ?? ''
}

```
</Tab>
<Tab title = "Python">
```python
from openai import AsyncOpenAI

openai_client = AsyncOpenAI(api_key=os.environ["OPENAI_API_KEY"])

async def get_capital(data):
    country = data["country"]
    response = await openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {
                "role": "user",
                "content": f"What is the capital of {country}? "
                "Just name the city and nothing else",
            },
        ],
    )
    return response.choices[0].message.content.strip()
```
</Tab>
</Tabs>

</Step>
<Step title="2. Define an evaluator function">

The evaluator function checks for exact match and returns 1 if the executor output 
matches the target, and 0 otherwise.

<Tabs>
<Tab title = "JavaScript/TypeScript">
```javascript
const evaluator = async (output: Promise<string>, target?: {capital: string}) =>
    (await output) === target?.capital ? 1 : 0
```
</Tab>
<Tab title = "Python">
```python
def evaluator(output, target):
    return 1 if output == target["capital"] else 0
```
</Tab>
</Tabs>

</Step>
<Step title="3. Define data and run the evaluation">

<Tabs>
<Tab title = "JavaScript/TypeScript">
```javascript my-eval.ts
import { evaluate } from '@lmnr-ai/lmnr';

const evaluationData = [
    { data: { country: 'Canada' }, target: { capital: 'Ottawa' } },
    { data: { country: 'Germany' }, target: { capital: 'Berlin' } },
    { data: { country: 'Tanzania' }, target: { capital: 'Dodoma' } },
]

evaluate({
    data: evaluationData,
    executor: async (data) => await getCapital(data),
    evaluators: { checkCapitalCorrectness: evaluator },
    config: {
        projectApiKey: process.env.LMNR_PROJECT_API_KEY
    }
})
```

And then run either `ts-node my-eval.ts` or `npx lmnr eval my-eval.ts`.
</Tab>
<Tab title = "Python">
```python my-eval.py
from lmnr import evaluate
import os

data = [
    {"data": {"country": "Germany"}, "target": {"capital": "Berlin"}},
    {"data": {"country": "Canada"}, "target": {"capital": "Ottawa"}},
    {"data": {"country": "Tanzania"}, "target": {"capital": "Dodoma"}},
]

evaluate(
    data=data,
    executor=get_capital,
    evaluators={'check_capital_correctness': evaluator},
    project_api_key=os.environ["LMNR_PROJECT_API_KEY"],
)

```

And then run either `python my-eval.py` or `lmnr eval my-eval.py`.
</Tab>
</Tabs>
</Step>
</Steps>

## LLM as a judge offline evaluation

In this example, our executor will write short summaries of news articles,
and the evaluator will check if the summary is correct, and grade them from 1 to 5.

<Steps>
<Step title="1. Prepare your data">

The trick here is that the evaluator function needs to see the original article to evaluate the summary.
That is why, we will have to duplicate the article from `data` into `target` prior to running the evaluation.

The data may look something like the following:

```json
[
    {
        "data": {
            "article": "Laminar has released a new feature. ...",
        },
        "target": {
            "article": "Laminar has released a new feature. ...",
        }
    }
]
```

</Step>
<Step title="2. Define an executor function">

An executor function calls OpenAI to summarize a news article. It returns a single string, the summary.

<Tabs>
<Tab title = "JavaScript/TypeScript">
```javascript
import OpenAI from 'openai';

const openai = new OpenAI({apiKey: process.env.OPENAI_API_KEY});

const getSummary = async (data: {article: string}): Promise<string> => {
    const response = await openai.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: [
            {
                role: "system",
                content: "Summarize the articles that the user sends you"
            }, {
                role: "user",
                content: data.article,
            },
        ],
    });
    return response.choices[0].message.content ?? ''
}

```
</Tab>
<Tab title = "Python">
```python
from openai import AsyncOpenAI

openai_client = AsyncOpenAI(api_key=os.environ["OPENAI_API_KEY"])

async def get_summary(data: dict[str, str]) -> str:
    response = await openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": "Summarize the articles that the user sends you"
            }, {
                "role": "user",
                "content": data["article"],
            },
        ],
    )
    return response.choices[0].message.content.strip()

```
</Tab>
</Tabs>

</Step>
<Step title="3. Define an evaluator function">

An evaluator function grades the summary from 1 to 5. It returns an integer.
We've simply asked OpenAI to respond in JSON, but you may want to use
structured output or BAML instead.

We also ask the LLM to give a comment on the summary. Even though we don't use it in the evaluation,
it may be useful for debugging or further analysis. In addition, LLMs are known to perform better
when given a chance to explain their reasoning.

<Tabs>
<Tab title = "JavaScript/TypeScript">
```javascript
import OpenAI from 'openai';

const openai = new OpenAI({apiKey: process.env.OPENAI_API_KEY});

const gradeSummary = async (
    summary: string,
    data: {article: string}
): Promise<number> => {
    const response = await openai.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: [{
            role: "user",
            content: "Given an article and its summary, grade the " +
                "summary from 1 to 5. Answer in json. For example: " +
                '{"grade": 3, "comment": "Summary is missing key points"}' +
                `Article: ${target['article']}. Summary: ${summary}`
        }],
    });
    return JSON.parse(response.choices[0].message.content ?? '')["grade"]
}
```
</Tab>
<Tab title = "Python">
```python
import json
async def grade_summary(summary: str, target: dict[str, str]) -> int:
    response = await openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{
            "role": "user",
            "content": "Given an article and its summary, grade the " +
              "summary from 1 to 5. Answer in json. For example: " +
              '{"grade": 3, "comment": "Summary is missing key points"}' +
              f"Article: {target['article']}. Summary: {summary}"
            },
        ],
    )
    return int(
        json.loads(response.choices[0].message.content.strip())["grade"]
    )

```
</Tab>
</Tabs>

</Step>
<Step title="4. Run the evaluation">

<Tabs>
<Tab title = "JavaScript/TypeScript">

```javascript my-eval.ts
import { evaluate } from '@lmnr-ai/lmnr';

const evaluationData = [
    { data: { article: '...' }, target: { article: '...' } },
    { data: { article: '...' }, target: { article: '...' } },
    { data: { article: '...' }, target: { article: '...' } },
]

evaluate({
    data: evaluationData,
    executor: async (data) => await getSummary(data),
    evaluators: { gradeSummary: gradeSummary },
    config: {
        projectApiKey: process.env.LMNR_PROJECT_API_KEY
    }
})
```

And then run either `ts-node my-eval.ts` or `npx lmnr eval my-eval.ts`.
</Tab>
<Tab title = "Python">
```python my-eval.py
from lmnr import evaluate
import os

data = [
    {"data": {"article": '...'}, "target": {"article": '...'}},
    {"data": {"article": '...'}, "target": {"article": '...'}},
    {"data": {"article": '...'}, "target": {"article": '...'}},
]

evaluate(
    data=data,
    executor=get_summary,
    evaluators={'grade_summary': grade_summary},
    project_api_key=os.environ["LMNR_PROJECT_API_KEY"],
)

```

And then run either `python my-eval.py` or `lmnr eval my-eval.py`.
</Tab>
</Tabs>
</Step>
</Steps>

## Evaluation with no target

Sometimes you may want to run evaluations on the output of the executor without a target.
This can be useful, for example, to check if the output of the executor is in the correct format
or if you want to use an LLM as a judge evaluator that generally evaluates the output.

<Tabs>
<Tab title = "JavaScript/TypeScript">

This is as simple as not passing `target` to your evaluator functions.

```javascript
function isOutputLongEnough(output) {
    return output.length > 100 ? 1 : 0
}
```
</Tab>
<Tab title = "Python">

In Python, you will have to add `*args, **kwargs` to your evaluator function.

```python
def is_output_long_enough(output, *args, **kwargs):
    return int(len(output) > 100)
```
</Tab>
</Tabs>

And for your dataset, you can just remove the `target` field. For example:

```json
[
    { "data": { "article": "..." } },
    { "data": { "article": "..." } },
    { "data": { "article": "..." } },
]
```
