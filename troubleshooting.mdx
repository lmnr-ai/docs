---
title: Troubleshooting
sidebarTitle: Troubleshooting
description: Resolve common tracing, instrumentation, and OpenTelemetry issues
---

## Tracing issues

## My JS auto-instrumentation is not working.

### Problem

I have instrumented my JavaScript code with Laminar, and I can see the manual spans in the Laminar UI, but I don't see the auto-instrumented spans.

### Cause

Laminar auto-instrumentations are based on OpenTelemetry's auto-instrumentations,
and they are optimized for `commonjs` modules.

### Solution

<Steps>
<Step title="Make sure that you pass `instrumentModules`">

The `instrumentModules` option is required to instrument the modules you need.

```javascript {7-11}
import { Laminar } from '@lmnr-ai/lmnr';
import { OpenAI } from 'openai';
import anthropic from 'anthropic';

Laminar.initialize({
  projectApiKey: process.env.LMNR_PROJECT_API_KEY,
  instrumentModules: {
    OpenAI: OpenAI,
    anthropic: anthropic,
    // ... other modules you want to instrument
  }
});
```

If this didn't solve your issue, continue with the next steps.
</Step>

<Step title="Check if you are using ESM modules">

Due to the differences between Node versions, bundlers, and TypeScript compilers, it is not easy to definitely determine if you are using ESM modules.

Some strong indicators:
- You have a `package.json` file with `"type": "module"` – this is a 100% indicator.
- Your files have `.mjs` extension – this is a strong indicator.
- You are using `import` statements **and** this is not TypeScript – this is a strong indicator.

See more in [Node.js documentation](https://nodejs.org/api/modules.html#enabling) and [Node.js documentation on determining module system](https://nodejs.org/api/packages.html#determining-module-system).

If you are not sure, you can try to run `node --input-type=module yourfile.js` and see if it works.

</Step>

<Step title="Try to migrate to CommonJS">

For modern Node versions, migrating to CommonJS is straightforward and most code actually
is written in ESM syntax.

It might be as simple as changing the file extension from `.mjs` to `.js` and adding `"type": "commonjs"` to your `package.json` file.

</Step>
<Step title="[Optional] Further reading">

Refer to the [CJS vs ESM](https://github.com/open-telemetry/opentelemetry-js/blob/main/doc/esm-support.md) guide
in OpenTelemetry documentation for more information.

</Step>
</Steps>

## My JS traces are not showing up in the Laminar UI.

### Problem

I have instrumented my JavaScript code with Laminar, but I don't see any traces in the Laminar UI.

### Cause

This often happens in Edge runtimes, Lambda functions, or in one-off scripts that are not running in a long-lived process.

JavaScript's OpenTelemetry batch span processor runs in a background async function, and, if the process exits before the function has a chance to
send the traces, they will be lost. In theory, there is a way to force flush the pending spans `onShutdown`, but it is not implemented in the
OpenTelemetry JS SDK. Apparently, doing that may cause the consequent incoming spans to block the process.

See [originating commit](https://github.com/open-telemetry/opentelemetry-js/commit/23db7f0ba383a3043964eb03be11c09df3f7453a) where `onShutdown` was only implemented for the browser contexts,
but not for Node.js.

### Solution

Use Laminar's `flush()` function to ensure that the traces are sent before the process exits.

```javascript {13}
import { Laminar } from '@lmnr-ai/lmnr';

Laminar.initialize({
  projectApiKey: process.env.LMNR_PROJECT_API_KEY,
  instrumentModules: {
    // your instrumented modules
  }
});

// your code here

// at the end of your script, when you are done tracing
await Laminar.flush(); // NOTE: don't forget to await
```

Learn more about flushing spans and shutting down Laminar tracing [here](/observe-in-production/privacy-and-tracing-control).

## My Python auto-instrumentation is not working.

### Problem

I have instrumented my Python code with Laminar, and I can see the manual spans in the Laminar UI, but I don't see the auto-instrumented spans.

### Cause

Most likely, you have not installed the right extras that enable the auto-instrumentations.

### Solution

Install the extras for LLM providers you are using and wish to instrument. For instance,

```sh
pip install lmnr[vertexai]
```

Read the [installation docs](/installation) for more information.

## I can see the LLM spans and token counts in the Laminar UI, but prices are 0.

### Problem

LLM spans are instrumented and token counts are shown, but the prices are all at 0.

### Cause

LLM providers do not return pricing information in the API responses, but they do return
token counts.

Laminar calculates the prices based on the model name and the provider name stored in the Laminar database.

### Solution

Make sure the model you are using is supported by Laminar.
View model formats and full list of models [here](/observe-in-production/cost-and-token-tracking#model-name-formats).

If the model/provider pair is not supported by Laminar, reach out to us and we'll add it.

## I can see traces and spans in the UI, but they are not showing inputs and/or outputs.

### Problem

I see traces and spans in the Laminar UI, but they are not showing the inputs and/or outputs. They have attributes and duration, but no inputs or outputs.

### Cause

One possible reason is that we send span inputs and outputs as span attributes, and OpenTelemetry has a limited set of possible attribute types, namely
`string`, `number`, `boolean`, and `array` of each. If the input or output is not one of these types, it will not be sent as an attribute.

To work around this, we serialize the input and output to JSON and send it as a string. We do our best to serialize the inputs and outputs, but it may be
that the serialization fails.

### Solution

Make sure inputs and outputs to your functions or spans are serializable to JSON. If you are using custom objects, make sure their fields are serializible.
In Python, that may mean having to implement `default()` method on some of them, or on the parent object.

## Many of my JS route requests are instrumented in the Laminar UI, even though I don't want that.

### Problem

I see many traces and spans in the UI, but most of them are just noise, they merely are route requests.

Some examples of noise spans:
- `GET /my/route`
- `POST /my/other/route`
- `dns.lookup`
- `middleware - query`
- `tcp - connect`
- `fs statSync`

### Cause

Even though Laminar only enables OpenLLMetry-specific instrumentations, i.e. just model calls, some other dependency in your code
may have enabled other instrumentations for Node libraries. In particular, we have seen this issue when such initialization is done
_before_ Laminar's initialization. For example:

```javascript
const { NodeSDK } = require('@opentelemetry/sdk-node');
const { getNodeAutoInstrumentations } = require(
  '@opentelemetry/auto-instrumentations-node'
);
const { BatchSpanProcessor, SamplingDecision } = require(
  '@opentelemetry/sdk-trace-base'
);

// this can also be not raw OpenTelemetry, but some other observability library
const sdk = new NodeSDK({
  instrumentations: [getNodeAutoInstrumentations()],
  spanProcessors: new BatchSpanProcessor(exporter),
});

const { Laminar } = require('@lmnr-ai/lmnr');
Laminar.initialize({ projectApiKey: "...", });
```

[Read more](https://github.com/open-telemetry/opentelemetry-js-contrib/tree/main/metapackages/auto-instrumentations-node#usage-auto-instrumentation) on OpenTelemetry's Node.js auto-instrumentations in OpenTelemetry.

### Solution

The most effective solution is to update lmnr-ai/lmnr to the latest version. Since version 0.6.0 Laminar has its own Tracer provider,
and it's not affected by other auto-instrumentations.

## SSL Certificate and GRPC Connection Issues for Laminar

### Problem

Experiencing errors related to SSL certificate verification and GRPC connections in Laminar. Error messages include `CERTIFICATE_VERIFY_FAILED` and `StatusCode.UNAVAILABLE` when attempting to export traces to a GRPC server.

### Cause

1. **SSL Certificate Verification Error**:
   - The certificate chain may be incomplete or incorrect.
   - The client is not configured to accept the server's certificate.

2. **GRPC Connection Error**:
   - SSL configuration issues prevent establishing a secure connection.
   - The server may actually be unavailable.

### Solution

To resolve these errors, follow these steps:

1. **Import the Certifi Library**:
   - Use the `certifi` library to obtain an updated set of trusted CA certificates.
   - Install using pip: `pip install certifi`

2. **Verify the Certifi Path**:
   - After installation, you can verify the path to the `certifi` CA bundle by running:
     ```
     python -m certifi
     ```
   - This command will print the path to the `cacert.pem` file, which you can use for further configurations.

3. **Configure Environment Variables**:
   - **`SSL_CERT_FILE`**: Set the path to the trusted CA certificate file.
     ```
     export SSL_CERT_FILE=$(python -m certifi)
     ```
   - **`GRPC_DEFAULT_SSL_ROOTS_FILE_PATH`**: Configure this variable so that GRPC uses the same CA certificate file.
     ```
     export GRPC_DEFAULT_SSL_ROOTS_FILE_PATH=$(python -m certifi)
     ```

## Observe decorator is not working in Python with async generators, i.e. model stream responses

### Problem

`@observe` decorator in Python is not correctly capturing the spans when I use async generators, e.g. when I am streaming responses from a model.

### Cause

Async generators only end when they are exhausted, i.e. when the `async for` loop is done.
Given the nature of Python's async, the chance of the interruptions within stream, and the
way OpenTelemetry's contexts work, there is no way to ensure that `@observe` works in 100% of the cases.

### Solution

One possible workaround is to observe only the outer functions that are calling the async generator, 
and handle the final result, not the generator itself.

However, if you want to track partial stream results as well, you can create the spans manually.
Refer to [manual instrumentation](/ship-to-production/manual-instrumentation) for more information.

## ERR_BUFFER_OUT_OF_BOUNDS in Node.js

### Problem

I see the following error in my Node.js application:

```sh
node:internal/buffer:1066
      throw new ERR_BUFFER_OUT_OF_BOUNDS('length');
      ^
```
<Expandable title="full error trace">
```sh
RangeError [ERR_BUFFER_OUT_OF_BOUNDS]: "length" is outside of buffer bounds
    at proto.utf8Write (node:internal/buffer:1066:13)
    at Op.writeStringBuffer [as fn] (/Users/sebastianscheibe/Code/XXX/node_modules/protobufjs/src/writer_buffer.js:61:13)
    at BufferWriter.finish (/Users/sebastianscheibe/Code/XXX/node_modules/protobufjs/src/writer.js:453:14)
    at /Users/sebastianscheibe/Code/XXX/node_modules/@grpc/proto-loader/build/src/index.js:177:109
    at Array.map (<anonymous>)
    at createPackageDefinition (/Users/sebastianscheibe/Code/XXX/node_modules/@grpc/proto-loader/build/src/index.js:177:39)
    at Object.fromJSON (/Users/sebastianscheibe/Code/XXX/node_modules/@grpc/proto-loader/build/src/index.js:230:12)
    at GrpcClient.loadProtoJSON (/Users/sebastianscheibe/Code/XXX/node_modules/google-gax/build/src/grpc.js:228:51)
    at new ImageAnnotatorClient (/Users/sebastianscheibe/Code/XXX/node_modules/@google-cloud/vision/build/src/v1/image_annotator_client.js:148:38)
    at Object.<anonymous> (/Users/sebastianscheibe/Code/XXX/test.js:5:16) {
  code: 'ERR_BUFFER_OUT_OF_BOUNDS'
}

Node.js v22.7.0
```
</Expandable>

### Cause

This is most likely due to a bug in NodeJS versions 22.6 and 22.7 with
handling utf-8 buffers. See more in the [Node.js issue](https://github.com/nodejs/node/issues/54518#issuecomment-2307035124).

### Solution

Upgrade to Node.js version 22.8.0 or higher. You can also downgrade to 22.5.

## OpenTelemetry troubleshooting

This page is relevant for the users not using Laminar's SDK for tracing, but sending
their OpenTelemetry traces to Laminar. Use the configuration steps in this section to connect your exporter.

## [Node.js / TypeScript] Error: 16 UNAUTHENTICATED: Failed to authenticate request

### Problem

I am using the OpenTelemetry Node.js SDK and I see the following error:

```sh
Error: 16 UNAUTHENTICATED: Failed to authenticate request
```

<Expandable title="full error trace">
```sh
Error: 16 UNAUTHENTICATED: Failed to authenticate request
    at callErrorFromStatus (/workspace/node_modules/.pnpm/@grpc+grpc-js@1.12.5/node_modules/@grpc/grpc-js/src/call.ts:82:17)
    at Object.onReceiveStatus (/workspace/node_modules/.pnpm/@grpc+grpc-js@1.12.5/node_modules/@grpc/grpc-js/src/client.ts:360:55)
    at Object.onReceiveStatus (/workspace/node_modules/.pnpm/@grpc+grpc-js@1.12.5/node_modules/@grpc/grpc-js/src/client-interceptors.ts:458:34)
    at Object.onReceiveStatus (/workspace/node_modules/.pnpm/@grpc+grpc-js@1.12.5/node_modules/@grpc/grpc-js/src/client-interceptors.ts:419:48)
    at /workspace/node_modules/.pnpm/@grpc+grpc-js@1.12.5/node_modules/@grpc/grpc-js/src/resolving-call.ts:163:24
    at processTicksAndRejections (node:internal/process/task_queues:85:11)
for call at
    at ServiceClientImpl.makeUnaryRequest (/workspace/node_modules/.pnpm/@grpc+grpc-js@1.12.5/node_modules/@grpc/grpc-js/src/client.ts:325:42)
    at ServiceClientImpl.export (/workspace/node_modules/.pnpm/@grpc+grpc-js@1.12.5/node_modules/@grpc/grpc-js/src/make-client.ts:189:15)
    at /workspace/node_modules/.pnpm/@opentelemetry+otlp-grpc-exporter-base@0.57.0_@opentelemetry+api@1.9.0/node_modules/@opentelemetry/otlp-grpc-exporter-base/src/grpc-exporter-transport.ts:159:26
    at new Promise (<anonymous>)
    at GrpcExporterTransport.send (/workspace/node_modules/.pnpm/@opentelemetry+otlp-grpc-exporter-base@0.57.0_@opentelemetry+api@1.9.0/node_modules/@opentelemetry/otlp-grpc-exporter-base/src/grpc-exporter-transport.ts:146:12)
    at OTLPExportDelegate.export (/workspace/node_modules/.pnpm/@opentelemetry+otlp-exporter-base@0.57.0_@opentelemetry+api@1.9.0/node_modules/@opentelemetry/otlp-exporter-base/src/otlp-export-delegate.ts:82:23)
    at OTLPTraceExporter.export (/workspace/node_modules/.pnpm/@opentelemetry+otlp-exporter-base@0.57.0_@opentelemetry+api@1.9.0/node_modules/@opentelemetry/otlp-exporter-base/src/OTLPExporterBase.ts:32:26)
    at doExport (/workspace/node_modules/.pnpm/@opentelemetry+sdk-trace-base@1.30.0_@opentelemetry+api@1.9.0/node_modules/@opentelemetry/sdk-trace-base/src/export/BatchSpanProcessorBase.ts:193:32)
    at /workspace/node_modules/.pnpm/@opentelemetry+sdk-trace-base@1.30.0_@opentelemetry+api@1.9.0/node_modules/@opentelemetry/sdk-trace-base/src/export/BatchSpanProcessorBase.ts:219:11
    at AsyncLocalStorage.run (node:internal/async_local_storage/async_hooks:91:14) {
  code: 16,
  details: 'Failed to authenticate request',
  metadata: Metadata {
    internalRepr: Map(3) {
      'content-type' => [Array],
      'content-length' => [Array],
      'date' => [Array]
    },
    options: {}
  }
}
```
</Expandable>

I have checked that my `LMNR_PROJECT_API_KEY` is correct and I set the headers when initializing the exporter.

### Cause

One common cause is that the `authorization` header on the gRPC exporter is set
as a raw HTTP header, instead of a gRPC metadata header.

### Solution

Make sure that the `authorization` header is passed to the exporter as part
of the metadata object.

```javascript {1,4-5,8}
import { Metadata } from '@grpc/grpc-js';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-grpc';

const metadata = new Metadata();
metadata.set('authorization', `Bearer ${process.env.LMNR_PROJECT_API_KEY}`);
const exporter = new OTLPTraceExporter({
  url: "https://api.lmnr.ai:8443/v1/traces",
  metadata,
});
```

## [Node.js / TypeScript] Error: Parse Error: Expected HTTP/

### Problem

I am using the OpenTelemetry Node.js SDK and I see the following error:

```sh
Error: Parse Error: Expected HTTP/'
```

<Expandable title="full error trace">
```sh
Error: Parse Error: Expected HTTP/'
Error: Parse Error: Expected HTTP/
    at Socket.socketOnData (node:_http_client:552:22)
    at Socket.emit (node:events:513:28)
    at Socket.emit (node:domain:489:12)
    at addChunk (node:internal/streams/readable:559:12)
    at readableAddChunkPushByteMode (node:internal/streams/readable:510:3)
    at Socket.Readable.push (node:internal/streams/readable:390:5)
    at TCP.onStreamRead (node:internal/stream_base_commons:189:23)
    at TCP.callbackTrampoline (node:internal/async_hooks:130:17) {
  bytesParsed: 0,
  code: 'HPE_INVALID_CONSTANT',
  reason: 'Expected HTTP/',
  rawPacket: <Buffer ...>
```
</Expandable>

### Cause

This error occurs when the exporter is configured to use the HTTP endpoint, but
sends traces to the gRPC endpoint. The error message indicates the mismatch
between the HTTP versions. gRPC uses HTTP/2.0, while the HTTP exporter uses
HTTP/1.1.

### Solution

Make sure that you are importing the gRPC exporter from the
`@opentelemetry/exporter-trace-otlp-grpc` package.

```javascript
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-grpc';
// not from `@opentelemetry/exporter-trace-otlp-proto`
// not from `@opentelemetry/exporter-trace-otlp-http`
```

## [Node.js / TypeScript] OTLPExporterError: Not Found

### Problem

I am using the OpenTelemetry Node.js SDK and I see the following error:

```sh
OTLPExporterError: Not Found
```

<Expandable title="full error trace">
```sh
 OTLPExporterError: Not Found
    at IncomingMessage.<anonymous> (/workspace/node_modules/.pnpm/@opentelemetry+otlp-exporter-base@0.57.0_@opentelemetry+api@1.9.0/node_modules/@opentelemetry/otlp-exporter-base/src/transport/http-transport-utils.ts:75:23)
    at IncomingMessage.emit (node:events:525:35)
    at IncomingMessage.emit (node:domain:489:12)
    at endReadableNT (node:internal/streams/readable:1696:12)
    at processTicksAndRejections (node:internal/process/task_queues:90:21) {
  data: '',
  code: 404
}
```
</Expandable>

### Cause

OTLP HTTP exporter called the correct base URL, but not the path.

### Solution

We recommend using the gRPC exporter,
as it is more reliable and faster. Make sure you are importing
`OTLPTraceExporter` from `@opentelemetry/exporter-trace-otlp-grpc`.

If you have to use the HTTP exporter, make sure that you are using the correct endpoint.
The endpoint for HTTP is `https://api.lmnr.ai:443/v1/traces` (port 443).

```javascript {1}
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';

const exporter = new OTLPSpanExporter({
    url: "https://api.lmnr.ai:443/v1/traces", // note the path `/v1/traces`
    headers: {
        Authorization: `Bearer ${process.env.LMNR_PROJECT_API_KEY}`,
    },
});
```

## [Node.js / TypeScript] OTLPExporterError: Internal Server Error

### Problem

I am using the OpenTelemetry Node.js SDK and I see the following error:

```sh
OTLPExporterError: Internal Server Error
```

<Expandable title="full error trace">
```sh
OTLPExporterError: Internal Server Error
    at IncomingMessage.<anonymous> (/workspace/node_modules/.pnpm/@opentelemetry+otlp-exporter-base@0.57.0_@opentelemetry+api@1.9.0/node_modules/@opentelemetry/otlp-exporter-base/src/transport/http-transport-utils.ts:75:23)
    at IncomingMessage.emit (node:events:525:35)
    at IncomingMessage.emit (node:domain:489:12)
    at endReadableNT (node:internal/streams/readable:1696:12)
    at processTicksAndRejections (node:internal/process/task_queues:90:21) {
  data: '',
  code: 500
}
```
</Expandable>

### Cause

While this may indicate any issue with the Laminar server, one of the most common cases
is attempting to send HTTP/json traces to HTTP/proto endpoint. Laminar does NOT support
HTTP/json traces.

### Solution

We recommend using the gRPC exporter,
as it is more reliable and faster. Make sure you are importing
`OTLPTraceExporter` from `@opentelemetry/exporter-trace-otlp-grpc`.

If you have to use the HTTP exporter, make sure that you are using are importing
`OTLPTraceExporter` from `@opentelemetry/exporter-trace-otlp-proto`, NOT from
`@opentelemetry/exporter-trace-otlp-http`, as the latter is for HTTP/json traces.

## [Python] TypeError: not all arguments converted during string formatting

### Problem

I am using the OpenTelemetry Python SDK and I see the following error:

```sh
TypeError: not all arguments converted during string formatting
```

<Expandable title="full error trace">
```sh
Traceback (most recent call last):
  File "/workspace/.venv/lib/python3.12/site-packages/opentelemetry/sdk/trace/export/__init__.py", line 360, in _export_batch
    self.span_exporter.export(self.spans_list[:idx])  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.12/site-packages/opentelemetry/exporter/otlp/proto/grpc/trace_exporter/__init__.py", line 143, in export
    return self._export(spans)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.12/site-packages/opentelemetry/exporter/otlp/proto/grpc/exporter.py", line 299, in _export
    self._client.Export(
  File "/workspace/.venv/lib/python3.12/site-packages/grpc/_channel.py", line 1178, in __call__
    ) = self._blocking(
        ^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.12/site-packages/grpc/_channel.py", line 1146, in _blocking
    call = self._channel.segregated_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi", line 547, in grpc._cython.cygrpc.Channel.segregated_call
  File "src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi", line 416, in grpc._cython.cygrpc._segregated_call
  File "src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi", line 410, in grpc._cython.cygrpc._segregated_call
  File "src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi", line 262, in grpc._cython.cygrpc._call
  File "src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi", line 305, in grpc._cython.cygrpc._call
  File "src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi", line 62, in grpc._cython.cygrpc._raise_call_error
  File "src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi", line 32, in grpc._cython.cygrpc._call_error
  File "src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi", line 23, in grpc._cython.cygrpc._call_error_metadata
TypeError: not all arguments converted during string formatting
```
</Expandable>

### Cause

This error indicates that some of the headers that you passed to the gRPC trace exporter
are not within the allowed set of keys. For Laminar uses, this is mosth likely the
`authorization` header.

### Solution

Check that the headers you pass to the trace exporter are valid. Most likely,
make sure that the `authorization` header starts with lowercase `a`.

```python {6-7}
import os
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

exporter = OTLPSpanExporter(
    endpoint="https://api.lmnr.ai:8443/v1/traces",
    # important: `authorization` starts with a lowercase letter
    headers={"authorization": f"Bearer {os.getenv('LMNR_PROJECT_API_KEY')}"},
)
```

## [Python] ConnectionResetError: [Errno 54] Connection reset by peer

### Problem

I am using the OpenTelemetry Python SDK and I see the following error:

```sh
ConnectionResetError: [Errno 54] Connection reset by peer
```

<Expandable title="full error trace">
```sh
Traceback (most recent call last):
  File "/workspace/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py", line 292, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 54] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspace/.venv/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.12/site-packages/urllib3/util/util.py", line 38, in reraise
    raise value.with_traceback(tb)
  File "/workspace/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py", line 292, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspace/.venv/lib/python3.12/site-packages/opentelemetry/sdk/trace/export/__init__.py", line 360, in _export_batch
    self.span_exporter.export(self.spans_list[:idx])  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.12/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py", line 189, in export
    return self._export_serialized_spans(serialized_data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.12/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py", line 159, in _export_serialized_spans
    resp = self._export(serialized_data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.12/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py", line 133, in _export
    return self._session.post(
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.12/site-packages/requests/adapters.py", line 682, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))
```
</Expandable>

### Cause

This error indicates that the connection was reset at Laminar's backend. Most likely,
this error indicates an HTTP version mismatch. One common cause for this is using
the HTTP exporter against the gRPC endpoint.

### Solution

Laminar accepts traces via both gRPC and HTTP. We recommend using the gRPC exporter,
as it is more reliable and faster. Make sure you are importing
`OTLPTraceExporter` from `opentelemetry.exporter.otlp.proto.grpc.trace_exporter` and not
from `opentelemetry.exporter.otlp.proto.http.trace_exporter`.

If you have to use the HTTP exporter, make sure that you are using the correct endpoint.
The endpoint for HTTP is `https://api.lmnr.ai:443/v1/traces` (port 443).

```python {5}
import os
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter

exporter = OTLPSpanExporter(
    endpoint="https://api.lmnr.ai:443/v1/traces",  # note the port 443
    headers={"authorization": f"Bearer {os.getenv('LMNR_PROJECT_API_KEY')}"},
)
```

## [Python] Failed to export batch code: 404, reason:

### Problem

I am using the OpenTelemetry Python SDK and I see the following error:

```sh
Failed to export batch code: 404, reason:
```

### Cause

OTLP HTTP exporter called the correct base URL, but not the path.

### Solution

We recommend using the gRPC exporter,
as it is more reliable and faster. Make sure you are importing
`OTLPTraceExporter` from `opentelemetry.exporter.otlp.proto.grpc.trace_exporter` and not
from `opentelemetry.exporter.otlp.proto.http.trace_exporter`.

If you have to use the HTTP exporter, make sure that you are using the correct endpoint.
The endpoint for HTTP is `https://api.lmnr.ai:443/v1/traces` (port 443).

```python {5}
import os
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter

exporter = OTLPSpanExporter(
    endpoint="https://api.lmnr.ai:443/v1/traces",  # note the path `/v1/traces`
    headers={"authorization": f"Bearer {os.getenv('LMNR_PROJECT_API_KEY')}"},
)
```
