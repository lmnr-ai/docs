---
title: Laminar
sidebarTitle: Laminar
---

See exactly what your AI is doing.

One import gives you full visibility into every LLM call, browser action, and agent step. Open-source. Self-hostable. Production-ready.

<CardGroup cols={2}>
  <Card title="Get started →" href="/tracing/quickstart">
    First trace in 60 seconds
  </Card>
  <Card title="GitHub" href="https://github.com/lmnr-ai/lmnr">
    Star us on GitHub
  </Card>
</CardGroup>

<Frame>
  <img src="/images/traces/traces.png" alt="Laminar trace view" />
</Frame>

## The 30-second version

<CodeGroup>
```typescript TypeScript
import 'lmnr/auto';  // ← This is all you need

// Your existing code works unchanged
const response = await openai.chat.completions.create({
  model: 'gpt-4o-mini',
  messages: [{ role: 'user', content: 'Hello!' }],
});
```

```python Python
import lmnr.auto  # ← This is all you need
from openai import OpenAI
client = OpenAI()

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello!"}]
)
```
</CodeGroup>

Open your dashboard. Every call appears with inputs, outputs, tokens, and cost.

## What you can do

<CardGroup cols={2}>
  <Card title="Trace LLM calls" href="/tracing/quickstart">
    Automatic instrumentation for OpenAI, Anthropic, Gemini, LangChain, and more.
  </Card>
  <Card title="Debug browser agents" href="/tracing/browser-agent-observability">
    Session recordings synchronized with traces. See exactly what your agent saw.
  </Card>
  <Card title="Run evaluations" href="/evaluations/quickstart">
    Score prompts and agents. Every datapoint links to its trace.
  </Card>
  <Card title="Query with SQL" href="/sql-editor/overview">
    Query traces, evals, and costs. Build dashboards or export datasets.
  </Card>
</CardGroup>

## Start here

1. [Quickstart](/tracing/quickstart) — first trace in 60 seconds  
2. [How Laminar works](/how-it-works) — architecture overview  
3. [Evaluations](/evaluations/quickstart) — score your AI
