---
sidebarTitle: Introduction
title: Datasets for evals and training
---

Datasets in Laminar hold the examples that power your evals, labeling, and training loops. Each datapoint is JSON you can version, filter, and connect back to traces.

<Callout>
Embed placeholder: screenshot of a dataset table with data/target/metadata columns and a “View trace” link.
</Callout>

## What you'll do with datasets

- Store inputs + expected outputs for evaluations.  
- Turn traces into labeled examples, then iterate in labeling queues.  
- Export query results (cost outliers, bad traces) straight into a dataset.

## Datapoint shape

```json
{
  "data": { "question": "What is the capital of France?" },
  "target": { "answer": "Paris" },
  "metadata": { "category": "geography" }
}
```

- `data`: the input to your executor.  
- `target`: optional reference passed to evaluators.  
- `metadata`: tags for filtering and grouping.

## Versioning (built-in)

- Every edit creates a new version with the same id and a new timestamp.  
- Reverting creates a new top version; history stays intact.  
- Sort with UUIDv7 to preserve insertion order.

## Common workflows

- **Feed evals**: wire a dataset into [evaluate](/evaluations/quickstart) to score prompts/agents.  
- **Label from traces**: push spans into a queue, label targets, and write back to the dataset.  
- **Export from SQL**: query outliers in the [SQL editor](/sql-editor/introduction) and export to a dataset.

<Callout>
Embed placeholder: GIF of selecting traces → exporting to dataset → running an eval.
</Callout>

## Build this next

- Create/load datasets programmatically → [Datasets CLI](/datasets/cli)  
- Label quickly → [Labeling queues](/queues/quickstart)  
- Run evals on your dataset → [Evaluations quickstart](/evaluations/quickstart)
