---
sidebarTitle: Introduction
title: Datasets for evals and training
---

Datasets in Laminar hold the examples that power your evals, labeling, and training loops. Each datapoint is JSON you can version, filter, and connect back to traces.

<Frame>
  <img src="/images/datasets/manual-add-dp.png" alt="Dataset datapoint view" />
</Frame>

## What you'll do with datasets

- Store inputs + expected outputs for evaluations.  
- Turn traces into labeled examples, then iterate in labeling queues.  
- Export query results (cost outliers, bad traces) straight into a dataset.

## Datapoint shape

```json
{
  "data": { "question": "What is the capital of France?" },
  "target": { "answer": "Paris" },
  "metadata": { "category": "geography" }
}
```

- `data`: the input to your executor.  
- `target`: optional reference passed to evaluators.  
- `metadata`: tags for filtering and grouping.

## Storage model (append-only)

- Each datapoint you add is stored as a row with the provided `id` (or a generated UUIDv7).  
- Updates today are append-only; there is no built-in version history or delete/rollback.  
- To change a datapoint, write a new row with the desired values; consumers should pick the record they need.

## Common workflows

- **Feed evals**: wire a dataset into [evaluate](/evaluations/quickstart) to score prompts/agents.  
- **Label from traces**: push spans into a queue, label targets, and write back to the dataset.  
- **Export from SQL**: query outliers in the [SQL editor](/sql-editor/overview) and export to a dataset.

<Frame>
  <img src="/images/datasets/export-span.png" alt="Export spans to dataset" />
</Frame>

## Build this next

- Create/load datasets programmatically → [Datasets CLI](/datasets/cli)  
- Label quickly → [Labeling queues](/queues/quickstart) (coming soon in the current server build)  
- Run evals on your dataset → [Evaluations quickstart](/evaluations/quickstart)
