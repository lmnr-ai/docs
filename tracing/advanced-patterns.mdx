---
title: Advanced Tracing Patterns
sidebarTitle: Advanced Tracing Patterns
description: Patterns for tracing across services, serverless functions, async workflows, and custom LLMs.
---

# Advanced Tracing Patterns

This page covers tracing in complex architectures: microservices, serverless functions, async workflows, and custom LLM integrations.

## Continuing Traces Across Services

When a single user request spans multiple services, you want one connected trace—not isolated fragments. Laminar supports this through span context serialization.

### Passing Context via HTTP Headers

The most common pattern. Service A serializes its span context and passes it to Service B:

```javascript
// Service A — serialize and send
import { Laminar, observe } from '@lmnr-ai/lmnr';

const callServiceB = () => observe({ name: 'call_service_b' }, async () => {
  const response = await fetch('https://service-b.internal/api/process', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'X-Laminar-Span-Context': Laminar.serializeLaminarSpanContext(),
    },
    body: JSON.stringify({ data: 'payload' }),
  });
  return response.json();
});
```

```javascript
// Service B — deserialize and continue
import { Laminar } from '@lmnr-ai/lmnr';
import express from 'express';

const app = express();

app.post('/api/process', (req, res) => {
  const parentContext = req.headers['x-laminar-span-context'];

  const span = Laminar.startSpan({
    name: 'service_b_handler',
    parentSpanContext: parentContext,
  });

  try {
    // Your processing logic
    const result = processRequest(req.body);
    res.json(result);
  } finally {
    span.end();
  }
});
```

```python
# Service A — serialize and send
import requests
from lmnr import Laminar, observe

@observe()
def call_service_b(payload: dict):
    span_context = Laminar.serialize_span_context()
    
    response = requests.post(
        'https://service-b.internal/api/process',
        headers={'X-Laminar-Span-Context': span_context},
        json=payload,
    )
    return response.json()
```

```python
# Service B — deserialize and continue
from flask import Flask, request
from lmnr import Laminar

app = Flask(__name__)

@app.route('/api/process', methods=['POST'])
def handle_request():
    span_context = request.headers.get('X-Laminar-Span-Context')
    parent = Laminar.deserialize_span_context(span_context) if span_context else None

    with Laminar.start_as_current_span(
        name='service_b_handler',
        parent_span_context=parent,
    ):
        result = process_request(request.json)
        return result
```

### Message Queue Pattern

For async processing where Service A enqueues work and Service B processes it later:

```python
# Producer — include trace context in message
@observe()
def enqueue_task(task_data: dict):
    span_context = Laminar.serialize_span_context()
    
    message = {
        'data': task_data,
        'trace_context': span_context,
    }
    queue.send(message)

# Consumer — continue the trace
def process_task(message: dict):
    parent = Laminar.deserialize_span_context(message.get('trace_context'))
    
    with Laminar.start_as_current_span(
        name='task_processed',
        parent_span_context=parent,
    ):
        # Process the task
        handle(message['data'])
```

### Database Storage Pattern

For long-running workflows that span multiple user sessions (e.g., approval flows, multi-day processes):

```python
# Start workflow — store context with workflow state
@observe()
def start_workflow(user_id: str, workflow_data: dict):
    span_context = Laminar.serialize_span_context()
    
    db.save_workflow({
        'user_id': user_id,
        'span_context': span_context,
        'data': workflow_data,
        'status': 'pending_approval',
    })

# Resume workflow — restore context
def continue_workflow(workflow_id: str):
    workflow = db.get_workflow(workflow_id)
    parent = Laminar.deserialize_span_context(workflow['span_context'])
    
    with Laminar.start_as_current_span(
        name='workflow_continued',
        parent_span_context=parent,
    ):
        # Continue processing
        finalize(workflow)
```

### Passing Spans Within a Service

For complex functions that need to share a parent span without relying on the call stack:

```javascript
import { Laminar } from '@lmnr-ai/lmnr';

const handleRequest = async (input) => {
  const rootSpan = Laminar.startSpan({ name: 'handle_request' });

  try {
    await processData(rootSpan, input);
    await generateResponse(rootSpan, input);
  } finally {
    rootSpan.end();
  }
};

const processData = async (span, data) => {
  await Laminar.withSpan(span, async () => {
    // Operations here are children of the passed span
    await validate(data);
  });
};

const generateResponse = async (span, data) => {
  await Laminar.withSpan(span, async () => {
    // Also children of the same parent span
    await callLLM(data);
  });
};
```

```python
from lmnr import Laminar, use_span

def handle_request(input_data):
    root_span = Laminar.start_span(name='handle_request')

    try:
        process_data(root_span, input_data)
        generate_response(root_span, input_data)
    finally:
        root_span.end()

def process_data(span, data):
    with use_span(span):
        # Operations here are children of the passed span
        validate(data)

def generate_response(span, data):
    with use_span(span):
        # Also children of the same parent span
        call_llm(data)
```

## Serverless and Lambda

Serverless functions terminate quickly, often before background processes finish. This can cause lost traces.

### The Problem

Laminar batches spans in memory and sends them in the background for performance. In serverless, the function may exit before the batch is sent.

### JavaScript — Use `await`

In Node.js Lambda functions, `flush()` and `shutdown()` are async. Just await them:

```javascript
import { Laminar } from '@lmnr-ai/lmnr';

export const handler = async (event) => {
  Laminar.initialize({ projectApiKey: process.env.LMNR_PROJECT_API_KEY });

  // Your logic here
  const result = await processEvent(event);

  await Laminar.flush();  // Wait for spans to send
  return result;
};
```

### Python — Use `force_flush()`

In Python, `flush()` is synchronous but the actual sending happens in a daemon thread. If the process exits, spans are lost. Use `force_flush()` instead:

```python
from lmnr import Laminar

def handler(event, context):
    Laminar.initialize()

    # Your logic here
    result = process_event(event)

    Laminar.force_flush()  # Blocks until spans are sent
    return result
```

`force_flush()` internally shuts down and reinitializes the SDK, ensuring spans are sent before returning. This is safe for Lambda's container reuse.

### When to Use What

| Scenario | JavaScript | Python |
|----------|------------|--------|
| Web server (long-running) | Nothing needed | Nothing needed |
| CLI script | `await Laminar.flush()` | `Laminar.flush()` |
| Lambda / serverless | `await Laminar.flush()` | `Laminar.force_flush()` |
| End of process | `await Laminar.shutdown()` | `Laminar.shutdown()` |

## Manual LLM Spans

Auto-instrumentation covers most providers. For unsupported providers or custom implementations, create LLM spans manually.

### When You Need This

- Custom or self-hosted models
- Providers Laminar doesn't auto-instrument yet
- Direct HTTP calls to LLM APIs

### Creating an LLM Span

Set `spanType: 'LLM'` and populate the required attributes:

```javascript
import { Laminar, LaminarAttributes } from '@lmnr-ai/lmnr';

const callCustomLLM = async (messages) => {
  const span = Laminar.startSpan({ name: 'custom_llm_call', spanType: 'LLM' });

  try {
    const response = await fetch('https://api.custom-llm.com/v1/completions', {
      method: 'POST',
      body: JSON.stringify({ model: 'custom-model-1', messages }),
    });
    const data = await response.json();

    span.setAttributes({
      [LaminarAttributes.PROVIDER]: 'custom-llm.com',
      [LaminarAttributes.REQUEST_MODEL]: 'custom-model-1',
      [LaminarAttributes.RESPONSE_MODEL]: data.model,
      [LaminarAttributes.INPUT_TOKEN_COUNT]: data.usage.input_tokens,
      [LaminarAttributes.OUTPUT_TOKEN_COUNT]: data.usage.output_tokens,
    });

    return data;
  } finally {
    span.end();
  }
};
```

```python
from lmnr import Laminar, Attributes
import requests

def call_custom_llm(messages: list):
    with Laminar.start_as_current_span(
        name='custom_llm_call',
        input=messages,
        span_type='LLM',
    ):
        response = requests.post(
            'https://api.custom-llm.com/v1/completions',
            json={'model': 'custom-model-1', 'messages': messages},
        ).json()

        Laminar.set_span_output(response['choices'][0]['message']['content'])
        Laminar.set_span_attributes({
            Attributes.PROVIDER: 'custom-llm.com',
            Attributes.REQUEST_MODEL: 'custom-model-1',
            Attributes.RESPONSE_MODEL: response['model'],
            Attributes.INPUT_TOKEN_COUNT: response['usage']['input_tokens'],
            Attributes.OUTPUT_TOKEN_COUNT: response['usage']['output_tokens'],
        })

        return response
```

### Required Attributes for Cost Tracking

For Laminar to calculate costs, set these attributes:

| Attribute | Description |
|-----------|-------------|
| `PROVIDER` | Provider name (e.g., `openai`, `anthropic`, `custom-llm.com`) |
| `RESPONSE_MODEL` | Model name as returned by the API |
| `INPUT_TOKEN_COUNT` | Number of input tokens |
| `OUTPUT_TOKEN_COUNT` | Number of output tokens |

See [supported providers](/providers) for the full list of recognized provider names.

## Privacy Controls

Sometimes you need to trace structure without capturing content—for sensitive operations, compliance requirements, or customer-specific privacy settings.

### Tracing Levels

| Level | What's Captured |
|-------|-----------------|
| `ALL` | Everything (default) |
| `META_ONLY` | Timing, token counts, costs—but no inputs/outputs |
| `OFF` | Nothing within the wrapper |

### Usage

```javascript
import { withTracingLevel, TracingLevel } from '@lmnr-ai/lmnr';

// For a specific operation
withTracingLevel(TracingLevel.META_ONLY, () => {
  // Inputs and outputs not recorded
  handleSensitiveData(userData);
});

// Normal tracing resumes here
```

```python
from lmnr import Laminar, TracingLevel

with Laminar.set_tracing_level(TracingLevel.META_ONLY):
    # Inputs and outputs not recorded
    handle_sensitive_data(user_data)

# Normal tracing resumes here
```

### Per-Customer Privacy

```python
@observe()
def process_request(customer_id: str, data: dict):
    customer = get_customer(customer_id)
    
    if customer.requires_privacy:
        with Laminar.set_tracing_level(TracingLevel.META_ONLY):
            return handle_request(data)
    else:
        return handle_request(data)
```

## Error Handling

Capture errors properly so they appear in traces:

```javascript
const span = Laminar.startSpan({ name: 'risky_operation' });

try {
  const result = await riskyOperation();
  span.setAttributes({ 'operation.status': 'success' });
  return result;
} catch (error) {
  span.recordException(error);
  throw error;
} finally {
  span.end();
}
```

```python
with Laminar.start_as_current_span(name='risky_operation') as span:
    try:
        result = risky_operation()
        span.set_attribute('operation.status', 'success')
        return result
    except Exception as error:
        span.record_exception(error)
        raise
```

## Best Practices

**Always end spans** — Use context managers (`with`) or `try/finally` to ensure spans end even on errors.

**Validate context before using** — Serialized context from external sources may be malformed:
```python
def safe_deserialize(context_str):
    try:
        return Laminar.deserialize_span_context(context_str) if context_str else None
    except Exception:
        return None  # Continue without parent context
```

**Don't flush in hot paths** — Never call `flush()` in web server request handlers. It's for scripts, CLI tools, and serverless only.

**Use `META_ONLY` for sensitive data** — When you need the trace structure but can't store content.
