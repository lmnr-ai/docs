---
title: Introduction
sidebarTitle: Introduction
description:
---

# Tracing

Tracing is the foundation of everything in Laminar. Every feature—debugging, evaluation, cost analysis, session replay—builds on trace data. Start here.

## What You Get

When you instrument your application, Laminar automatically captures:

- **Execution flow**: See exactly how requests move through your AI agent, which functions call which, and where time is spent
- **LLM details**: Every prompt, completion, model used, token count, and cost
- **Performance**: Latency at each step, bottlenecks, slow operations
- **Errors**: Where things failed and why

This data powers everything else: you can't evaluate what you can't see, and you can't improve what you can't measure.

## Key Concepts

**Trace**: A complete execution path through your application—one user request, one agent run, one workflow execution.

**Span**: A single operation within a trace. An LLM call is a span. A function you wrap with `observe` is a span. Spans nest to show parent-child relationships.

**Session**: A way to group related traces together, like all the turns in a multi-message conversation.

## Get Started

### 1. Initialize Laminar

Grab your API key from your [project settings](https://laminar.ai) and add two lines to your application entry point:

```javascript
import { Laminar } from '@lmnr-ai/lmnr';
import OpenAI from 'openai';

Laminar.initialize({
    projectApiKey: process.env.LMNR_PROJECT_API_KEY,
    instrumentModules: { openAI: OpenAI }
});
```

```python
from lmnr import Laminar

Laminar.initialize()  # reads LMNR_PROJECT_API_KEY from environment
```

### 2. Use Your LLM as Normal

```javascript
const client = new OpenAI();
const response = await client.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [{ role: "user", content: "What is the capital of France?" }],
});
```

```python
from openai import OpenAI

client = OpenAI()
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "What is the capital of France?"}],
)
```

That's it. This call appears in your dashboard with full details—latency, tokens, cost, the complete prompt and response.

### What Gets Captured

Automatic instrumentation records:

- **LLM calls**: Request parameters, response content, token usage, latency, cost
- **Framework operations**: Chain executions, agent reasoning steps, tool calls
- **Vector DB queries**: Similarity searches, retrieval results
- **Errors**: Exceptions, retry attempts, rate limiting

### 3. Add Structure with `observe`

For deeper visibility, wrap your own functions:

```javascript
import { observe } from '@lmnr-ai/lmnr';

const answerQuestion = (question) => observe(
  { name: 'answer_question' },
  async () => {
    const response = await client.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [{ role: "user", content: question }],
    });
    return response.choices[0].message.content;
  }
);

await answerQuestion("What is the capital of France?");
```

```python
from lmnr import observe

@observe()
def answer_question(question: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": question}],
    )
    return response.choices[0].message.content

answer_question("What is the capital of France?")
```

Now you see `answer_question` as the parent operation with the OpenAI call nested inside it. Function inputs and outputs are captured automatically.

## Enrich Your Traces

Add context to make traces filterable and meaningful:

**Sessions** — Group related traces together (e.g., all turns in a conversation):
```javascript
Laminar.setTraceSessionId('conversation-123');
```

**User ID** — Associate traces with specific users:
```javascript
Laminar.setTraceUserId('user-456');
```

**Metadata** — Add key-value context for filtering:
```javascript
Laminar.setTraceMetadata({ environment: 'production', feature: 'chat' });
```

**Tags** — Categorize individual spans:
```javascript
Laminar.setSpanTags(['high-priority', 'beta-user']);
```

All of these must be called within a span context (inside an `observe` wrapper or manual span).

## Supported Integrations

Laminar auto-instruments OpenAI, Anthropic, Gemini, LangChain, Vercel AI SDK, Playwright, Pinecone, and 20+ other libraries. See [Integrations](/tracing/integrations) for the full list and setup details.

## Real-Time Visibility

Traces stream to your dashboard as they execute—you don't wait for operations to complete. This is particularly useful when debugging long-running agents or chained LLM calls.

By default, traces are batched for performance. For immediate visibility during development, disable batching with `disableBatch: true` in your initialization config.

You can also toggle this in your project settings on the dashboard.

## Key Concepts

**Trace**: A complete execution path through your application—one user request, one agent run, one workflow execution.

**Span**: A single operation within a trace. An LLM call is a span. A function you wrap with `observe` is a span. Spans nest to show parent-child relationships.

**Session**: A way to group related traces together, like all the turns in a multi-message conversation.

## Next Steps

- [SDK Reference](/sdk/reference): Full API details for all tracing methods
- [Evaluations](/evaluations/introduction): Score and test your traces automatically
