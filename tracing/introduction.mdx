---
title: Tracing
sidebarTitle: Introduction
description:
---

Tracing records what your AI agent does—every LLM call, every tool use, every decision—so you can see exactly what happened and why.

## Quick Start

### 1. Initialize Laminar

Grab your API key from your [project settings](https://www.lmnr.ai/projects) and add this to your application entry point:

<Tabs>
<Tab title="Python">

```python
from lmnr import Laminar

Laminar.initialize(project_api_key="your-key")
```

</Tab>
<Tab title="TypeScript">

```typescript
import { Laminar } from '@lmnr-ai/lmnr';
import OpenAI from 'openai';

Laminar.initialize({
  projectApiKey: process.env.LMNR_PROJECT_API_KEY,
  instrumentModules: { openAI: OpenAI }
});
```

</Tab>
</Tabs>

### 2. Use Your LLM as Normal

<Tabs>
<Tab title="Python">

```python
from openai import OpenAI

client = OpenAI()
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "What is the capital of France?"}],
)
```

</Tab>
<Tab title="TypeScript">

```typescript
const client = new OpenAI();
const response = await client.chat.completions.create({
  model: "gpt-4o-mini",
  messages: [{ role: "user", content: "What is the capital of France?" }],
});
```

</Tab>
</Tabs>

That's it. This call appears in your [dashboard](https://www.lmnr.ai) with the full prompt, response, token count, latency, and cost.

---

## What Just Happened?

- Laminar intercepted your OpenAI call and recorded it as a **span**—a single operation with a start time, end time, inputs, and outputs.
- A request usually involves multiple spans (LLM calls, tools, routing). Together they form a **trace**—the full tree of what happened for one request.

![Trace example](/images/trace.png)

This view shows where time goes—here a single span ran for **1.67s** and used **21** tokens.

### What Laminar Records Automatically

When you call `initialize()`, Laminar watches for calls to supported libraries and records them without any code changes:

- **LLM calls** — Prompts, completions, model name, tokens, cost, latency
- **Framework operations** — LangChain chains, agent steps, tool invocations
- **Vector databases** — Queries, retrieved documents, similarity scores

This automatic recording is called *instrumentation*. You can also manually instrument your own functions—see [Tracing Structure](/tracing/structure/overview).

---

## Grouping Traces into Sessions

A trace is one request. But conversations have many requests—a user sends a message, your agent responds, they send another message.

**Sessions** group related traces together. Attach a session ID to your traces, and you can view an entire conversation as a unit:

<Tabs>
<Tab title="Python">

```python
from lmnr import observe

@observe(session_id="conversation-123")
def handle_message(user_input):
    # Each call creates a trace, all grouped under the same session
    ...
```

</Tab>
<Tab title="TypeScript">

```typescript
import { observe } from '@lmnr-ai/lmnr';

await observe(
  { name: 'handle_message', sessionId: 'conversation-123' },
  async () => {
    // Each call creates a trace, all grouped under the same session
  }
);
```

</Tab>
</Tabs>

Sessions also enable [session replay](/tracing/structure/sessions)—you can watch a conversation unfold step by step.
