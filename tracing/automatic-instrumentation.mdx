---
title: Automatic LLM tracing
sidebarTitle: Automatic LLM tracing
description: Automatically trace LLM calls with zero code changes
---

import AutoInstrumentableJSModules from "/snippets/autoinstrumentable-js-modules.mdx"

## Overview

Automatic instrumentation is the easiest way to start tracing your LLM applications. When enabled, Laminar automatically captures:

- **LLM API calls** with request/response data, token usage, and costs
- **Vector database operations** for retrieval and storage
- **Framework calls** from LangChain, LlamaIndex, and other libraries
- **Request metadata** like latency, errors, and model parameters

<Tabs>
<Tab title="Python">
**Default behavior**: All supported libraries are automatically instrumented when you call `Laminar.initialize()`.

```python
from lmnr import Laminar
from openai import OpenAI

# Enable automatic instrumentation for all supported libraries
Laminar.initialize(project_api_key="your-api-key")

# All OpenAI calls are now automatically traced
client = OpenAI()
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello!"}]
)
```
</Tab>
<Tab title="JavaScript/TypeScript">
**Recommended approach**: Specify which modules to instrument using the `instrumentModules` parameter.

```javascript
import { Laminar } from '@lmnr-ai/lmnr';
import { OpenAI } from 'openai';

// Enable automatic instrumentation for specific modules
Laminar.initialize({
  projectApiKey: process.env.LMNR_PROJECT_API_KEY,
  instrumentModules: {
    openai: OpenAI
  }
});

// All OpenAI calls are now automatically traced
const client = new OpenAI();
const response = await client.chat.completions.create({
  model: "gpt-4o-mini",
  messages: [{ role: "user", content: "Hello!" }]
});
```
</Tab>
</Tabs>

## Enable All Libraries

This approach instruments all supported libraries automatically.

<Tabs>
<Tab title="Python">

```python
from lmnr import Laminar
from openai import OpenAI
from anthropic import Anthropic

# Automatically instrument all supported libraries
Laminar.initialize(project_api_key="your-api-key")

# All LLM calls from any supported library are now traced
openai_client = OpenAI()
anthropic_client = Anthropic()
```

<Note>
This is the default behavior in Python. No additional configuration needed.
</Note>

</Tab>
<Tab title="JavaScript/TypeScript">

```javascript
import { Laminar } from '@lmnr-ai/lmnr';

// Initialize before importing LLM libraries
Laminar.initialize({
  projectApiKey: process.env.LMNR_PROJECT_API_KEY
});

// Import after initialization
import { OpenAI } from 'openai';
import Anthropic from '@anthropic-ai/sdk';
```

<Warning>
This approach may not work with all bundlers. If you encounter issues, use [selective instrumentation](#instrument-specific-libraries) instead.
</Warning>

</Tab>
</Tabs>

## Instrument Specific Libraries

For better control and compatibility, instrument only the libraries you need.

<Tabs>
<Tab title="JavaScript/TypeScript">

**Recommended approach** for JavaScript/TypeScript applications:

```javascript
import { OpenAI } from 'openai';
import Anthropic from '@anthropic-ai/sdk';
import { Laminar } from '@lmnr-ai/lmnr';

Laminar.initialize({
  projectApiKey: process.env.LMNR_PROJECT_API_KEY,
  instrumentModules: {
    openai: OpenAI,
    anthropic: Anthropic
  }
});

// Both OpenAI and Anthropic calls are now traced
const openaiClient = new OpenAI();
const anthropicClient = new Anthropic();
```

**Selective instrumentation** (instrument only some libraries):

```javascript
import { OpenAI } from 'openai';
import Anthropic from '@anthropic-ai/sdk';
import { Laminar } from '@lmnr-ai/lmnr';

Laminar.initialize({
  projectApiKey: process.env.LMNR_PROJECT_API_KEY,
  instrumentModules: {
    anthropic: Anthropic // Only Anthropic calls are traced
  }
});

const openaiClient = new OpenAI();     // NOT traced
const anthropicClient = new Anthropic(); // Traced ✓
```

</Tab>
<Tab title="Python">

Use the `instruments` parameter to specify which libraries to instrument:

```python
from lmnr import Laminar, Instruments
from openai import OpenAI
from anthropic import Anthropic

# Instrument specific libraries only
Laminar.initialize(
    project_api_key="your-api-key",
    instruments={
        Instruments.OPENAI,
        Instruments.ANTHROPIC
    }
)

openai_client = OpenAI()     # Traced ✓
anthropic_client = Anthropic() # Traced ✓
```

**Selective instrumentation** example:

```python
from lmnr import Laminar, Instruments

# Only instrument Anthropic calls
Laminar.initialize(
    project_api_key="your-api-key",
    instruments={Instruments.ANTHROPIC}
)

openai_client = OpenAI()     # NOT traced
anthropic_client = Anthropic() # Traced ✓
```

</Tab>
</Tabs>

## Disable Instrumentation

To disable automatic instrumentation entirely:

<Tabs>
<Tab title="JavaScript/TypeScript">

```javascript
import { Laminar } from '@lmnr-ai/lmnr';

Laminar.initialize({
  projectApiKey: process.env.LMNR_PROJECT_API_KEY,
  instrumentModules: {} // Empty object = no instrumentation
});

// No LLM calls will be automatically traced
// Use manual instrumentation instead
```

</Tab>
<Tab title="Python">

```python
from lmnr import Laminar

Laminar.initialize(
    project_api_key="your-api-key",
    instruments=set() # Empty set = no instrumentation
)

# No LLM calls will be automatically traced
# Use manual instrumentation instead
```

</Tab>
</Tabs>

## Supported Libraries

Laminar supports automatic instrumentation for a wide range of libraries:

<Tabs>
<Tab title="JavaScript/TypeScript">

**LLM Providers:**
- OpenAI (`openai`)
- Anthropic (`@anthropic-ai/sdk`)
- Google AI (`@google/generative-ai`)
- Cohere (`cohere-ai`)
- Replicate (`replicate`)

**Frameworks:**
- Vercel AI SDK (`ai`)
- LangChain (`langchain`, `@langchain/core`)

**Vector Databases:**
- Pinecone (`@pinecone-database/pinecone`)
- Qdrant (`@qdrant/js-client-rest`)

<Expandable title="View complete list of supported modules">
<AutoInstrumentableJSModules />
</Expandable>

</Tab>
<Tab title="Python">

**LLM Providers:**
- OpenAI (`openai`)
- Anthropic (`anthropic`)
- Google AI (`google-generativeai`)
- Cohere (`cohere`)
- Ollama (`ollama`)
- Together AI (`together`)

**Frameworks:**
- LangChain (`langchain`)
- LlamaIndex (`llama-index`)
- DSPy (`dspy`)

**Vector Databases:**
- Pinecone (`pinecone-client`)
- Qdrant (`qdrant-client`)
- Chroma (`chromadb`)
- Weaviate (`weaviate-client`)

For the complete list, import `Instruments` from `lmnr` or [view the source](https://github.com/lmnr-ai/lmnr-python/blob/main/src/lmnr/opentelemetry_lib/tracing/instruments.py).

</Tab>
</Tabs>

## Integration-Specific Guides

Some frameworks require additional configuration:

- **Next.js applications**: See the [Next.js integration guide](/tracing/integrations/nextjs)
- **Vercel AI SDK**: See the [Vercel AI SDK guide](/tracing/integrations/vercel-ai-sdk)
- **LangChain**: See the [LangChain integration guide](/tracing/integrations/langchain)

## What Gets Traced

When automatic instrumentation is enabled, you'll see detailed traces including:

### LLM Calls
- Request parameters (model, messages, temperature, etc.)
- Response content and metadata
- Token usage (input, output, total)
- Latency and performance metrics
- Automatic cost calculation

### Framework Operations
- Chain executions in LangChain
- Agent reasoning steps
- Tool calls and results
- Vector similarity searches

### Error Handling
- Exception details and stack traces
- Retry attempts and failures
- Rate limiting and quota errors

## Next Steps

Once automatic instrumentation is working:

1. **Add structure** with the [`observe` decorator](/tracing/structure/observe) to group related operations
2. **Organize traces** into [sessions](/tracing/structure/session) for multi-turn conversations  
3. **Add metadata** for better filtering and analysis
4. **Set up evaluations** to monitor quality and performance

Automatic instrumentation provides comprehensive observability with minimal setup, making it easy to understand and optimize your LLM applications.
