---
title: Observe your Next.js app (App Router)
sidebarTitle: Next.js
description: Instrument Next.js with Laminar and trace your routes and LLM calls in minutes.
---

See your Next.js routes, LLM calls, and AI SDK telemetry in Laminar with minimal config. This recipe uses App Router.

<Callout>
Embed placeholder: GIF of a Next.js route calling an LLM and the resulting trace tree with cost.
</Callout>

## What you'll build

- A Next.js app with Laminar tracing initialized once.  
- Traces for route handlers and AI SDK calls (inputs, outputs, tokens, cost).  
- Playground links on spans to iterate on prompts.

## Copy/paste/run

<Steps>
<Step title="Install Laminar">

```bash
npm add @lmnr-ai/lmnr
```

</Step>
<Step title="Expose Laminar to Next.js (required for OpenTelemetry)">

`next.config.ts`:

```ts
const nextConfig = {
  serverExternalPackages: ['@lmnr-ai/lmnr'],
};

export default nextConfig;
```

</Step>
<Step title="Initialize Laminar once">

`instrumentation.ts`:

```ts {3-4,6-8}
export async function register() {
  if (process.env.NEXT_RUNTIME === 'nodejs') {
    const { Laminar } = await import('@lmnr-ai/lmnr');
    Laminar.initialize({
      projectApiKey: process.env.LMNR_PROJECT_API_KEY,
    });
  }
}
```

</Step>
<Step title="Trace your route + LLM call">

```ts app/api/chat/route.ts {1-2,6-17}
import { NextResponse, NextRequest } from 'next/server';
import { observe } from '@lmnr-ai/lmnr';
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

export const POST = observe(async (req: NextRequest) => {
  const body = await req.json();
  const { text } = await generateText({
    model: openai('gpt-4o-mini'),
    prompt: body?.prompt ?? 'Hello from Laminar',
  });

  return NextResponse.json({ text });
});
```

</Step>
</Steps>

<Callout>
Embed placeholder: trace view showing `POST /api/chat` with `ai.generateText` as a child span, including inputs/outputs and cost.
</Callout>

## What to look for

- Route span (`POST /api/chat`) with duration.  
- Child span for AI SDK with prompt/response, tokens, and cost.  
- Playground button on the LLM span to iterate on the prompt.

## Why this matters

- Fast visibility into routes and model calls without custom logging.  
- Cost and latency by route help you prioritize optimizations.  
- Shareable trace links make it easy to collaborate on prompt changes.

## Build this next

- Use the AI SDK tracer directly → [Vercel AI SDK integration](/tracing/integrations/vercel-ai-sdk)  
- Compare providers in one app → [OpenAI](/tracing/integrations/openai), [Anthropic](/tracing/integrations/anthropic)  
- Add tags to break down costs by feature/team → [Tags and metadata](/tracing/structure/tags)
