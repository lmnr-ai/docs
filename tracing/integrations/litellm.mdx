---
title: LLM Observability for LiteLLM
sidebarTitle: LiteLLM
description: Configure LiteLLM to send traces to Laminar
---

## Overview

[LiteLLM](https://www.litellm.ai/) is a framework/library for building LLM applications that simplifies accessing many models across different providers.

## Default configuration

LiteLLM is well integrated with OpenTelemetry, so you only need to specify the
configuration through the environment variables.

<Steps>
<Step title="Install the opentelemetry packages">
Laminar comes with required opentelemetry packages, so you only need to install Laminar:

```sh
pip install 'lmnr[all]'
```

If you want to install the opentelemetry packages separately, follow the instructions below.

<Expandable title="advanced installation - raw opentelemetry packages">

```sh
pip install opentelemetry-api opentelemetry-sdk\
  opentelemetry-exporter-otlp
```

</Expandable>

</Step>
<Step title="install LiteLLM">
For opentelemetry callback to work, you need to install LiteLLM with the `proxy` extra of the LiteLLM package.

```sh
pip install 'litellm[proxy]'
```

</Step>
<Step title="Set the environment variables">

```bash
LMNR_PROJECT_API_KEY="<your-project-api-key>"
OTEL_EXPORTER="otlp_grpc"
OTEL_ENDPOINT="https://api.lmnr.ai:8443"
OTEL_HEADERS="authorization=Bearer $LMNR_PROJECT_API_KEY"
```

<Note>
`authorization` must start with a lowercase `a`, because gRPC headers are case-sensitive
in Python OpenTelemetry SDK.
</Note>

</Step>
<Step title="Enable otel callback in the code">
```python
import litellm
litellm.callbacks = ['otel']
```
</Step>
<Step title="Run your code and see traces in Laminar">

Example code:

```python
import litellm
litellm.callbacks = ['otel']

 response = litellm.completion(
    model="gpt-4.1-nano",
    messages=[
      {"role": "user", "content": "What is the capital of France?"}
    ],
)
```


<div style={{ border: '1px solid #2B2B30', borderRadius: '8px', overflow: 'hidden' }}>
  <img src="/images/traces/litellm.png" alt="LiteLLM trace" style={{ margin: '0px' }}/>
</div>
</Step>
</Steps>


## Using Laminar's features

If you want to use Laminar's features, such as sessions, manual spans, and the `observe` decorator,
you will need to install and initialize Laminar alongside setting LiteLLM's callback.

<Steps>
<Step title="Install Laminar">
```sh
pip install lmnr
```
</Step>
<Step title="Initialize Laminar">
```python {1,4,6}
from lmnr import Laminar, observe
import litellm

Laminar.initialize(project_api_key="LMNR_PROJECT_API_KEY")
litellm.callbacks = ['otel']

@observe()
def completion(model, messages):
    response = litellm.completion(
        model=model,
        messages=messages,
    )
    return response

completion(
  "gpt-4.1-nano",
  [{"role": "user", "content": "What is the capital of France?"}]
)
```
</Step>
</Steps>

This, however, will most likely result in your OpenAI calls being double-traced â€“ once by LiteLLM and once by Laminar.
This is because LiteLLM uses OpenAI SDK under the hood to call some of the models and Laminar instruments OpenAI SDK.

To avoid this, you can disable OpenAI instrumentation at initialization.

```python {4}
from lmnr import Laminar, Instruments
Laminar.initialize(
  project_api_key="LMNR_PROJECT_API_KEY",
  disabled_instruments=set([Instruments.OPENAI])
)
```
