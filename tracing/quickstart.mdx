---
sidebarTitle: Quickstart
title: Quickstart
---

Get your first trace in 60 seconds.

## 1) Install

<CodeGroup>
```bash npm
npm install @lmnr-ai/lmnr openai
```

```bash pip
pip install 'lmnr[openai]'
```
</CodeGroup>

## 2) Initialize once

<CodeGroup>
```typescript TypeScript
import { Laminar } from '@lmnr-ai/lmnr';
import { OpenAI } from 'openai';

Laminar.initialize({
  projectApiKey: process.env.LMNR_PROJECT_API_KEY,
  instrumentModules: { OpenAI },
});

const client = new OpenAI();
const response = await client.chat.completions.create({
  model: 'gpt-4o-mini',
  messages: [{ role: 'user', content: 'Hello!' }],
});
console.log(response.choices[0].message.content);
```

```python Python
import os
from lmnr import Laminar
from openai import OpenAI

Laminar.initialize(project_api_key=os.environ["LMNR_PROJECT_API_KEY"])
client = OpenAI()

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello!"}]
)
print(response.choices[0].message.content)
```
</CodeGroup>

## 3) Run with your API key

<CodeGroup>
```bash TypeScript
LMNR_PROJECT_API_KEY=your_key node app.js
```

```bash Python
LMNR_PROJECT_API_KEY=your_key python app.py
```
</CodeGroup>

## 4) Open your dashboard

Go to [lmnr.ai/projects](https://lmnr.ai/projects) and click your trace.

<Frame>
  <img src="/images/traces/trace-openai.png" alt="OpenAI span with inputs, outputs, tokens, and cost" />
</Frame>

You’ll see:
- Full input prompt and complete response
- Token count and cost
- Latency breakdown

## What just happened?

- `Laminar.initialize(...)` patches supported SDKs before they’re used (OpenAI, Anthropic, etc.).
- Each call sends inputs, outputs, tokens, cost, and timing to your Laminar project.
- The dashboard renders spans as a trace tree; every span links to Playground for quick iteration.

## Add structure with custom spans

Wrap functions to see them as parent spans:

<CodeGroup>
```typescript TypeScript
import { observe } from '@lmnr-ai/lmnr';

const classifyTicket = (ticket: string) =>
  observe({ name: 'classify_ticket' }, async () => {
    return await client.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: `Classify: ${ticket}` }],
    });
  });
```

```python Python
from lmnr import observe

@observe()
def classify_ticket(ticket: str):
    return client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"Classify: {ticket}"}]
    )
```
</CodeGroup>

Now your trace shows:

```
classify_ticket
└── openai.chat.completions.create
```

<Frame>
  <img src="/images/traces/trace-observe.png" alt="Trace with parent span" />
</Frame>

## Next steps

<CardGroup cols={2}>
  <Card title="Track users & sessions" href="/tracing/structure/session">
    Group traces by user/session
  </Card>
  <Card title="Run evaluations" href="/evaluations/quickstart">
    Score prompts and agents automatically
  </Card>
  <Card title="Debug browser agents" href="/tracing/browser-agent-observability">
    See recordings synced to traces
  </Card>
  <Card title="Add metadata & tags" href="/tracing/structure/tags">
    Filter by environment, feature, team
  </Card>
</CardGroup>
