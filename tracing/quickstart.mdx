---
title: Quickstart
description: Get started with Laminar Tracing in 2 minutes.
---

import LaminarInitialize from '/snippets/laminar-initialize.mdx';
import GetProjectApiKey from '/snippets/get-project-api-key.mdx';
import ObserveExample from '/snippets/observe-example.mdx';

## Getting Started in 2 Minutes

### 1. Get Your Project API Key

To get your Project API Key, navigate to your project settings page on the Laminar dashboard and create new project API key.

Next, you'll need to set this key as an environment variable in your project. Create a `.env` file in the root of your project (if you don't have one already) and add the following line:

```env
LMNR_PROJECT_API_KEY=your_project_api_key_here
```

Replace `your_project_api_key_here` with the actual key you copied.

### 2. Initialize Laminar in Your Application

Adding just two lines to your application enables comprehensive tracing:

<LaminarInitialize />

### 3. That's it! Your LLM API Calls Are Now Traced

Once initialized, Laminar automatically traces LLM API calls. For example, after initialization, this standard OpenAI call:

<Tabs>
<Tab title="JavaScript/TypeScript">
```javascript
import { OpenAI } from 'openai';

const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const response = await client.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [{ role: "user", content: "What is the capital of France?" }],
});
```
</Tab>
<Tab title="Python">

```python
from openai import OpenAI

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "What is the capital of France?"}],
)
```
</Tab>
</Tabs>

Will automatically create a span in your Laminar dashboard:

<div style={{ border: '1px solid #2B2B30', borderRadius: '8px', overflow: 'hidden' }}>
  <img src="/images/traces/trace-openai.png" alt="OpenAI span visualization" style={{ margin: '0px' }}/>
</div>

<Note>
Laminar automatically captures important LLM metrics including latency, token usage, and cost calculations based on the specific model used.
</Note>

## Tracing Custom Functions

Beyond automatic LLM tracing, you can use the `observe` decorator/wrapper to trace specific functions in your application:

<ObserveExample />

## Next Steps

- Explore our integrations to see how Laminar works with your favorite tools:
    - [OpenAI](/tracing/integrations/openai)
    - [Anthropic](/tracing/integrations/anthropic)
    - [Gemini](/tracing/integrations/gemini)
    - [Langchain](/tracing/integrations/langchain)
    - [Next.js](/tracing/integrations/nextjs)
    - [Vercel AI SDK](/tracing/integrations/vercel-ai-sdk)
    - [LiteLLM](/tracing/integrations/litellm)
    - [Playwright](/tracing/integrations/playwright)
    - [Browser Use](/tracing/integrations/browser-use)
    - [Stagehand](/tracing/integrations/stagehand)
    - [Puppeteer](/tracing/integrations/puppeteer)
- Continue to [Trace Structure](/tracing/structure) to learn more about adding structure to your traces.
- Explore [Browser agent observability](/tracing/browser-agent-observability) to trace browser sessions and agent execution steps.
- If you want to get into details on OpenTelemetry, check out the in-depth [OpenTelemetry guide](/tracing/otel). 