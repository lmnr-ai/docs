---
sidebarTitle: Quickstart
title: Get your first Laminar trace in 3 commands
---

Laminar traces your LLM calls and functions with almost no setup. Follow the copy–paste–run path below to get a live trace (inputs, outputs, tokens, and cost) in under 5 minutes.

<Callout>
Embed placeholder: live trace viewer or GIF showing a span tree expanding with cost + token breakdown.
</Callout>

## What you'll build

- A captured trace for a single LLM call (inputs/outputs, latency, tokens, cost).
- Optional custom span wrapping your code so you can see parent/child relationships.
- A shareable trace link you can open in Playgrounds for fast iteration.

## Three commands to first trace

<Tabs>
<Tab title="JavaScript/TypeScript">

```bash
# 1) Install Laminar + OpenAI client
npm install @lmnr-ai/lmnr openai

# 2) Add one-line auto-instrumentation (import anywhere before your app code)
echo "import 'lmnr/auto'" > bootstrap.ts

# 3) Run any OpenAI script with your Laminar key
LMNR_PROJECT_API_KEY=your_key node app.js
```

Example app (drop below your import):

```ts
import 'lmnr/auto';
import { OpenAI } from 'openai';

const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const response = await client.chat.completions.create({
  model: 'gpt-4o-mini',
  messages: [{ role: 'user', content: 'What is the capital of France?' }],
});

console.log(response.choices[0].message.content);
```

</Tab>
<Tab title="Python">

```bash
# 1) Install
pip install lmnr openai

# 2) One-line auto-instrumentation
echo "import lmnr.auto" > bootstrap.py

# 3) Run any OpenAI script with your Laminar key
LMNR_PROJECT_API_KEY=your_key python app.py
```

Example app:

```python
import lmnr.auto  # traces OpenAI + your functions
from openai import OpenAI
import os

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

resp = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "What is the capital of France?"}],
)

print(resp.choices[0].message.content)
```

</Tab>
</Tabs>

<Note>
You can still call `Laminar.initialize()` for advanced options (custom names, module-specific instrumentation, tags), but the auto-import above is the fastest path to value.
</Note>

## Add a custom span (optional, 1 line)

Wrap any function to see it as a parent span above your LLM calls.

<Tabs>
<Tab title="JavaScript/TypeScript">

```ts
import { observe } from '@lmnr-ai/lmnr';

const classify = () =>
  observe({ name: 'classify_ticket' }, async () => {
    return await client.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: 'Classify this ticket: payment bug' }],
    });
  });

await classify();
```

</Tab>
<Tab title="Python">

```python
from lmnr import observe

@observe(name="classify_ticket")
def classify():
    return client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": "Classify this ticket: payment bug"}],
    )

classify()
```

</Tab>
</Tabs>

<Callout>
Embed placeholder: screenshot/GIF of the resulting trace tree showing `classify_ticket` as the parent span and the OpenAI call as a child, with token + cost totals.
</Callout>

## Why this matters

- **Faster onboarding**: no project scaffolding or SDK boilerplate before seeing value.
- **Trustworthy debugging**: inputs/outputs + tokens/costs live with the call stack, not split across logs.
- **Shareable context**: copy a trace URL to loop in teammates or jump into Playgrounds to iterate.

## Build this next

- Debug a browser agent with session recordings → [Browser agent observability](/tracing/browser-agent-observability)
- Trace a LangGraph/Stagehand workflow → [LangGraph visualization](/tracing/langgraph-visualization)
- Run evals on a RAG pipeline → [Evaluations quickstart](/evaluations/quickstart)
- Break down costs by feature/team → [Tags and metadata](/tracing/structure/tags)
